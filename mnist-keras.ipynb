{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the Train time and Inference Time of MNIST MLP and \n",
    "# CNN on CPUs and GPUs with `keras`\n",
    "\n",
    "The purpose of this notebook is to determine the relative speeds of training MNIST on CPUs and GPUs, as well as the relative speeds of inference. We will do this both for Convolutional Neural Network (CNN) and for Multi-Layer Perceptron (MLP) implementations of MNIST. It is similar to one of the experiments described in [this paper](https://arxiv.org/pdf/1904.08986.pdf) (arXiv:1904.08986v1 \\[physics.data-an\\]). The next step in this process will be to transform the code in this notebook into bare `tensorflow` code and compare the runtime between that and this `keras` implementation, and again between CPUs and GPUs in bare `tensorflow`. Then we will set the GPU implementation of MNIST up as a service. Ultimately, we look forward to running MNIST on TPUs and comparing runtime again.\n",
    "\n",
    "The code for the implementations of mnist using MLP with `keras` were pulled from [this github](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py), and the code for the CNN impementation was pulled from [this file](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) on the same github. I significantly changed both so that they would be more comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We import all the necessary classes and set some of the globals for the program. `NUM_CLASSES` is the number of categories to train mnist on. `NUM_EPOCHS` describes the number of epochs to run training over. `IMG_EDGE` is the side length of one of the (square) images, making the total pixel cound `IMG_EDGE ** 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adadelta\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Disable depreciation warnings\n",
    "try:\n",
    "    from tensorflow.python.util import deprecation\n",
    "    deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "except AttributeError:\n",
    "    print(\"Import failed\")\n",
    "\n",
    "# Set global constants\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 5\n",
    "IMG_EDGE = 28\n",
    "INFERENCE_TIME_THRESHOLD=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating MNIST\n",
    "\n",
    "Having imported all the necessary modules and defined the constants, we need to implement MNIST in `keras`. Since we plan to implement MNIST with MLP and with CNN, we will first create a parent class holding the common functions of both. Later, we will define two subclasses, one for MLP and once for CNN. The parent class has several functions:\n",
    "- `_load` and `_finish_load` load the default `keras` MNIST dataset in whatever form MLP or CNN wants the data to be in.\n",
    "- `_get_batch_sizes` sets a list of all the batch sizes to test. Because CNN uses so much memory, we will need to test smaller batch sizes or else the machine will crash. However, for the MLP implementation, we can train on very high batch sizes. Hence the batch sizes will be different for each case.\n",
    "- `_create` creates the MNIST model with a specific batch size.\n",
    "- `_load_inferences` loads several randomly generated images to be inferred on.\n",
    "- `_train` trains the MNIST model, keeping track of the time it takes to do so, and returns that time. The time it returns is actually the time to train _per iteration_, where one iteration is the number of epochs times the number of data points divided by the batch size.\n",
    "- `_predict` runs a number of inferences equal to the batch size the model was trained on and returns the time per inference. It does this multiple times to reduce uncertainty.\n",
    "- `get_data` runs all of the above functions in order to get the train times and inference times for the given machine type and implementation (MLP or CNN) for all the batch sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, machine):\n",
    "        self.machine = machine\n",
    "        self.model = None\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
    "        self.start_power = 0\n",
    "        self.end_power = 0\n",
    "        self.train_times = []\n",
    "        self.inference_times = []\n",
    "        self.batch_sizes = []\n",
    "        \n",
    "    def _load(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _load_inferences(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _get_batch_sizes(self):\n",
    "        for i in range(self.start_power, self.end_power):\n",
    "            self.batch_sizes += list(range(10**i, 10**(i+1), 10**i))\n",
    "        self.batch_sizes += [10**self.end_power]\n",
    "    \n",
    "    def _finish_load(self):\n",
    "        self.x_train = self.x_train.astype('float32')\n",
    "        self.x_test = self.x_test.astype('float32')\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        print('Train dataset size:', self.x_train.shape[0])\n",
    "        print('Test dataset size:', self.x_test.shape[0])\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = to_categorical(self.y_train, NUM_CLASSES)\n",
    "        self.y_test = to_categorical(self.y_test, NUM_CLASSES)\n",
    "    \n",
    "    def _create(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _train(self, batch_size):\n",
    "        start_time = time()\n",
    "        history = self.model.fit(self.x_train, self.y_train, batch_size=batch_size, epochs=NUM_EPOCHS, verbose=1,\n",
    "                            validation_data=(self.x_test, self.y_test))\n",
    "        end_time = time()\n",
    "        train_time = (end_time - start_time) / (NUM_EPOCHS * self.x_train.shape[0])\n",
    "\n",
    "        #loss, accuracy = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        return train_time\n",
    "    \n",
    "    def _predict(self, batch_size):\n",
    "        inference_time = 0\n",
    "        start_inference = time()\n",
    "        inference_num = 0\n",
    "        while True: # Do multiple trials\n",
    "            inputs = self._load_inferences()\n",
    "\n",
    "            start_time = time()\n",
    "            self.model.predict(inputs, batch_size=batch_size)\n",
    "            end_time = time()\n",
    "            \n",
    "            inference_time += end_time - start_time\n",
    "            inference_num += 1\n",
    "            if end_time - start_inference > INFERENCE_TIME_THRESHOLD:\n",
    "                print(\"Done inference\")\n",
    "                break\n",
    "        return inference_time / (inference_num * self.x_train.shape[0]), inference_num\n",
    "    \n",
    "    def get_data(self):\n",
    "        self.max_train = 0\n",
    "        self.max_inference = 0\n",
    "        \n",
    "        self._load()\n",
    "        self._get_batch_sizes()\n",
    "        for batch_size in self.batch_sizes:\n",
    "            self._create()\n",
    "            \n",
    "            train_time = self._train(batch_size)\n",
    "            inference_time, inference_num = self._predict(batch_size)\n",
    "            print('\\n','Batch size:', batch_size, '\\tTrain time:', train_time, '\\tInference time', inference_time, '(%s)'%inference_num)\n",
    "            print('+'*100)\n",
    "            self.train_times.append(train_time)\n",
    "            self.inference_times.append(inference_time)\n",
    "            \n",
    "            K.clear_session()# Clean up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our superclass, we may create two subclasses, one for MLP and one for CNN. They each handle the data and create the model differently, but in all other respects, `keras` allows us to treat them similarly, hence the superclass functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLP(MNIST):\n",
    "    def _load(self):\n",
    "        self.x_train = self.x_train.reshape(60000, IMG_EDGE**2)\n",
    "        self.x_test = self.x_test.reshape(10000, IMG_EDGE**2)\n",
    "        \n",
    "        self.start_power = 0\n",
    "        self.end_power = 4\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "    def _load_inferences(self):\n",
    "        return np.random.rand(self.x_train.shape[0], IMG_EDGE**2)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(512, activation='relu'))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=RMSprop(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(MNIST):\n",
    "    def _load(self):\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], 1, IMG_EDGE, IMG_EDGE)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], 1, IMG_EDGE, IMG_EDGE)\n",
    "            self.input_shape = (1, IMG_EDGE, IMG_EDGE)\n",
    "        else:\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "            self.input_shape = (IMG_EDGE, IMG_EDGE, 1)\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "        self.start_power = 0\n",
    "        self.end_power = 2# Smaller because CNN takes up more memory\n",
    "        \n",
    "    def _load_inferences(self):\n",
    "        return np.random.rand(self.x_train.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "            self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(128, activation='relu'))\n",
    "            self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=Adadelta(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Now that we have defined all the methods we need to gather data on the train time and inference time of MNIST on different machines with different implementations, all we need to do is call the functions. `get_data` will generate our lists of train times and inference times for every batch size. This will take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST MLP\n",
      "\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 282s 5ms/step - loss: 0.7742 - acc: 0.9178 - val_loss: 0.7470 - val_acc: 0.9400\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 269s 4ms/step - loss: 0.6824 - acc: 0.9454 - val_loss: 0.5939 - val_acc: 0.9545\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 274s 5ms/step - loss: 0.5790 - acc: 0.9540 - val_loss: 0.5413 - val_acc: 0.9585\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 276s 5ms/step - loss: 0.5345 - acc: 0.9591 - val_loss: 0.4262 - val_acc: 0.9669\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 289s 5ms/step - loss: 0.4898 - acc: 0.9624 - val_loss: 0.4375 - val_acc: 0.9666\n",
      "Done inference\n",
      "\n",
      " Batch size: 1 \tTrain time: 0.00463285360733668 \tInference time 0.0005607285221417745 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.5780 - acc: 0.9224 - val_loss: 0.5016 - val_acc: 0.9509\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 188s 3ms/step - loss: 0.5566 - acc: 0.9500 - val_loss: 0.4475 - val_acc: 0.9631\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.5362 - acc: 0.9570 - val_loss: 0.4830 - val_acc: 0.9615\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 173s 3ms/step - loss: 0.5034 - acc: 0.9596 - val_loss: 0.4701 - val_acc: 0.9636\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 175s 3ms/step - loss: 0.4833 - acc: 0.9634 - val_loss: 0.4769 - val_acc: 0.9636\n",
      "Done inference\n",
      "\n",
      " Batch size: 2 \tTrain time: 0.0030870140480995178 \tInference time 0.00039501887957255045 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.4849 - acc: 0.9254 - val_loss: 0.4609 - val_acc: 0.9515\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.4855 - acc: 0.9504 - val_loss: 0.4255 - val_acc: 0.9559\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 0.4666 - acc: 0.9581 - val_loss: 0.3554 - val_acc: 0.9690\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.4682 - acc: 0.9603 - val_loss: 0.3657 - val_acc: 0.9687\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.4352 - acc: 0.9635 - val_loss: 0.3345 - val_acc: 0.9727\n",
      "Done inference\n",
      "\n",
      " Batch size: 3 \tTrain time: 0.0019886515307426453 \tInference time 0.00030668368736902875 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.4077 - acc: 0.9258 - val_loss: 0.4552 - val_acc: 0.9479\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.4423 - acc: 0.9510 - val_loss: 0.3903 - val_acc: 0.9604\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.4290 - acc: 0.9583 - val_loss: 0.4088 - val_acc: 0.9628\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.4496 - acc: 0.9621 - val_loss: 0.4833 - val_acc: 0.9574\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.4178 - acc: 0.9652 - val_loss: 0.2939 - val_acc: 0.9738\n",
      "Done inference\n",
      "\n",
      " Batch size: 4 \tTrain time: 0.0015024498089154562 \tInference time 0.00019445170958836872 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 78s 1ms/step - loss: 0.3737 - acc: 0.9284 - val_loss: 0.2453 - val_acc: 0.9576\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.3908 - acc: 0.9511 - val_loss: 0.3431 - val_acc: 0.9622\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.3941 - acc: 0.9580 - val_loss: 0.3403 - val_acc: 0.9660\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 0.3864 - acc: 0.9618 - val_loss: 0.3812 - val_acc: 0.9667\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 0.4094 - acc: 0.9635 - val_loss: 0.3869 - val_acc: 0.9689\n",
      "Done inference\n",
      "\n",
      " Batch size: 5 \tTrain time: 0.0011833875234921773 \tInference time 0.00016256376504898072 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 60s 1ms/step - loss: 0.3482 - acc: 0.9289 - val_loss: 0.2935 - val_acc: 0.9610\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.3606 - acc: 0.9524 - val_loss: 0.2793 - val_acc: 0.9643\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 0.3665 - acc: 0.9593 - val_loss: 0.2799 - val_acc: 0.9687\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.3815 - acc: 0.9611 - val_loss: 0.3160 - val_acc: 0.9686\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.3572 - acc: 0.9655 - val_loss: 0.3714 - val_acc: 0.9651\n",
      "Done inference\n",
      "\n",
      " Batch size: 6 \tTrain time: 0.0011670885038375855 \tInference time 0.00016871166229248048 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 0.3258 - acc: 0.9295 - val_loss: 0.1866 - val_acc: 0.9627\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.3239 - acc: 0.9552 - val_loss: 0.2114 - val_acc: 0.9665\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 0.3421 - acc: 0.9587 - val_loss: 0.2907 - val_acc: 0.9657\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 65s 1ms/step - loss: 0.3326 - acc: 0.9629 - val_loss: 0.3245 - val_acc: 0.9674\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 0.3472 - acc: 0.9645 - val_loss: 0.2866 - val_acc: 0.9737\n",
      "Done inference\n",
      "\n",
      " Batch size: 7 \tTrain time: 0.0011120547199249267 \tInference time 0.00014571091334025064 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 56s 928us/step - loss: 0.3109 - acc: 0.9301 - val_loss: 0.2042 - val_acc: 0.9627\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 51s 856us/step - loss: 0.3025 - acc: 0.9556 - val_loss: 0.2027 - val_acc: 0.9692\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 56s 941us/step - loss: 0.3171 - acc: 0.9607 - val_loss: 0.3392 - val_acc: 0.9620\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 57s 943us/step - loss: 0.3056 - acc: 0.9645 - val_loss: 0.2798 - val_acc: 0.9706\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.3147 - acc: 0.9678 - val_loss: 0.2940 - val_acc: 0.9715\n",
      "Done inference\n",
      "\n",
      " Batch size: 8 \tTrain time: 0.0009452949754397075 \tInference time 0.00012852662007013956 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 54s 907us/step - loss: 0.2842 - acc: 0.9326 - val_loss: 0.1858 - val_acc: 0.9661\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 49s 818us/step - loss: 0.2754 - acc: 0.9564 - val_loss: 0.2787 - val_acc: 0.9665\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 48s 801us/step - loss: 0.2883 - acc: 0.9620 - val_loss: 0.2025 - val_acc: 0.9720\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 45s 747us/step - loss: 0.2962 - acc: 0.9646 - val_loss: 0.2971 - val_acc: 0.9691\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 48s 795us/step - loss: 0.3102 - acc: 0.9677 - val_loss: 0.2778 - val_acc: 0.9707\n",
      "Done inference\n",
      "\n",
      " Batch size: 9 \tTrain time: 0.0008144742250442505 \tInference time 0.00012006571491559346 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 48s 799us/step - loss: 0.2845 - acc: 0.9314 - val_loss: 0.1786 - val_acc: 0.9648\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 50s 832us/step - loss: 0.2616 - acc: 0.9567 - val_loss: 0.2792 - val_acc: 0.9579\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 51s 850us/step - loss: 0.2647 - acc: 0.9627 - val_loss: 0.2650 - val_acc: 0.9669\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 47s 777us/step - loss: 0.2730 - acc: 0.9657 - val_loss: 0.2485 - val_acc: 0.9670\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 45s 746us/step - loss: 0.2805 - acc: 0.9669 - val_loss: 0.2302 - val_acc: 0.9711\n",
      "Done inference\n",
      "\n",
      " Batch size: 10 \tTrain time: 0.0008023828832308452 \tInference time 0.0001123758614063263 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 24s 401us/step - loss: 0.2331 - acc: 0.9345 - val_loss: 0.1466 - val_acc: 0.9662\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 27s 447us/step - loss: 0.1647 - acc: 0.9647 - val_loss: 0.1488 - val_acc: 0.9711\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 29s 481us/step - loss: 0.1692 - acc: 0.9673 - val_loss: 0.1593 - val_acc: 0.9689\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 28s 464us/step - loss: 0.1702 - acc: 0.9712 - val_loss: 0.1540 - val_acc: 0.9734\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 28s 467us/step - loss: 0.1703 - acc: 0.9724 - val_loss: 0.1669 - val_acc: 0.9760\n",
      "Done inference\n",
      "\n",
      " Batch size: 20 \tTrain time: 0.0004529981780052185 \tInference time 7.745010852813721e-05 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 23s 383us/step - loss: 0.2281 - acc: 0.9338 - val_loss: 0.1343 - val_acc: 0.9635\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 22s 374us/step - loss: 0.1382 - acc: 0.9654 - val_loss: 0.1092 - val_acc: 0.9724\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 25s 417us/step - loss: 0.1261 - acc: 0.9714 - val_loss: 0.1492 - val_acc: 0.9733\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.1231 - acc: 0.9741 - val_loss: 0.1448 - val_acc: 0.9738\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 0.1180 - acc: 0.9773 - val_loss: 0.1392 - val_acc: 0.9752\n",
      "Done inference\n",
      "\n",
      " Batch size: 30 \tTrain time: 0.00038472362995147706 \tInference time 6.926493909623888e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.2224 - acc: 0.9339 - val_loss: 0.1104 - val_acc: 0.9675\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.1223 - acc: 0.9681 - val_loss: 0.1195 - val_acc: 0.9700\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.1088 - acc: 0.9730 - val_loss: 0.1400 - val_acc: 0.9705\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.1031 - acc: 0.9764 - val_loss: 0.1127 - val_acc: 0.9758\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.0961 - acc: 0.9788 - val_loss: 0.1248 - val_acc: 0.9760\n",
      "Done inference\n",
      "\n",
      " Batch size: 40 \tTrain time: 0.00029175181786219277 \tInference time 5.931078460481432e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.2218 - acc: 0.9330 - val_loss: 0.1108 - val_acc: 0.9652\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: 0.1108 - acc: 0.9693 - val_loss: 0.1035 - val_acc: 0.9723\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 0.0937 - acc: 0.9757 - val_loss: 0.1080 - val_acc: 0.9768\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.0881 - acc: 0.9792 - val_loss: 0.1009 - val_acc: 0.9794\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0802 - acc: 0.9810 - val_loss: 0.1088 - val_acc: 0.9785\n",
      "Done inference\n",
      "\n",
      " Batch size: 50 \tTrain time: 0.00029828756809234617 \tInference time 6.317333380381266e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.2207 - acc: 0.9326 - val_loss: 0.1188 - val_acc: 0.9652\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.1082 - acc: 0.9691 - val_loss: 0.1065 - val_acc: 0.9714\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.0897 - acc: 0.9754 - val_loss: 0.1024 - val_acc: 0.9749\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0750 - acc: 0.9800 - val_loss: 0.1197 - val_acc: 0.9694\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0700 - acc: 0.9825 - val_loss: 0.0989 - val_acc: 0.9794\n",
      "Done inference\n",
      "\n",
      " Batch size: 60 \tTrain time: 0.0002544802149136861 \tInference time 5.5756955676608616e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.2254 - acc: 0.9306 - val_loss: 0.1042 - val_acc: 0.9676\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.1064 - acc: 0.9689 - val_loss: 0.0961 - val_acc: 0.9741\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0808 - acc: 0.9763 - val_loss: 0.0852 - val_acc: 0.9764\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0682 - acc: 0.9809 - val_loss: 0.0877 - val_acc: 0.9785\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0610 - acc: 0.9835 - val_loss: 0.0865 - val_acc: 0.9799\n",
      "Done inference\n",
      "\n",
      " Batch size: 70 \tTrain time: 0.00024396814902623493 \tInference time 5.8638136916690405e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.2317 - acc: 0.9302 - val_loss: 0.0936 - val_acc: 0.9719\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.1042 - acc: 0.9688 - val_loss: 0.0783 - val_acc: 0.9764\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0798 - acc: 0.9767 - val_loss: 0.0910 - val_acc: 0.9744\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0668 - acc: 0.9816 - val_loss: 0.0754 - val_acc: 0.9796\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0595 - acc: 0.9841 - val_loss: 0.0837 - val_acc: 0.9805\n",
      "Done inference\n",
      "\n",
      " Batch size: 80 \tTrain time: 0.00022888947010040283 \tInference time 5.417464044358995e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.2316 - acc: 0.9286 - val_loss: 0.1097 - val_acc: 0.9638\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.1010 - acc: 0.9692 - val_loss: 0.0807 - val_acc: 0.9760\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.0785 - acc: 0.9771 - val_loss: 0.1058 - val_acc: 0.9728\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.0667 - acc: 0.9813 - val_loss: 0.0908 - val_acc: 0.9781\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0580 - acc: 0.9843 - val_loss: 0.0828 - val_acc: 0.9817\n",
      "Done inference\n",
      "\n",
      " Batch size: 90 \tTrain time: 0.0002142064627011617 \tInference time 5.1415328184763594e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 0.2318 - acc: 0.9295 - val_loss: 0.1052 - val_acc: 0.9670\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.1009 - acc: 0.9694 - val_loss: 0.0896 - val_acc: 0.9732\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0748 - acc: 0.9777 - val_loss: 0.0788 - val_acc: 0.9795\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0620 - acc: 0.9817 - val_loss: 0.0786 - val_acc: 0.9784\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0540 - acc: 0.9843 - val_loss: 0.0743 - val_acc: 0.9815\n",
      "Done inference\n",
      "\n",
      " Batch size: 100 \tTrain time: 0.000197178529103597 \tInference time 5.233177741368612e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.2711 - acc: 0.9170 - val_loss: 0.1274 - val_acc: 0.9586\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.1070 - acc: 0.9667 - val_loss: 0.0792 - val_acc: 0.9770\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0769 - acc: 0.9768 - val_loss: 0.0736 - val_acc: 0.9782\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0574 - acc: 0.9815 - val_loss: 0.0674 - val_acc: 0.9796\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0482 - acc: 0.9845 - val_loss: 0.0712 - val_acc: 0.9808\n",
      "Done inference\n",
      "\n",
      " Batch size: 200 \tTrain time: 0.0001714441657066345 \tInference time 4.712630377875434e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.3132 - acc: 0.9037 - val_loss: 0.1409 - val_acc: 0.9550\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.1178 - acc: 0.9626 - val_loss: 0.0861 - val_acc: 0.9737\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0796 - acc: 0.9754 - val_loss: 0.0704 - val_acc: 0.9783\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0616 - acc: 0.9807 - val_loss: 0.0748 - val_acc: 0.9783\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0498 - acc: 0.9847 - val_loss: 0.0719 - val_acc: 0.9786\n",
      "Done inference\n",
      "\n",
      " Batch size: 300 \tTrain time: 0.0001579070727030436 \tInference time 4.622253179550171e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.3424 - acc: 0.8944 - val_loss: 0.1354 - val_acc: 0.9586\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1265 - acc: 0.9619 - val_loss: 0.0969 - val_acc: 0.9699\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0846 - acc: 0.9734 - val_loss: 0.0752 - val_acc: 0.9764\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0628 - acc: 0.9804 - val_loss: 0.0664 - val_acc: 0.9809\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0498 - acc: 0.9845 - val_loss: 0.0676 - val_acc: 0.9806\n",
      "Done inference\n",
      "\n",
      " Batch size: 400 \tTrain time: 0.00015446025053660075 \tInference time 4.541888369454278e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.3738 - acc: 0.8848 - val_loss: 0.1444 - val_acc: 0.9531\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1377 - acc: 0.9581 - val_loss: 0.1143 - val_acc: 0.9620\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0921 - acc: 0.9714 - val_loss: 0.0842 - val_acc: 0.9740\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0689 - acc: 0.9783 - val_loss: 0.0733 - val_acc: 0.9771\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0528 - acc: 0.9834 - val_loss: 0.0672 - val_acc: 0.9792\n",
      "Done inference\n",
      "\n",
      " Batch size: 500 \tTrain time: 0.00014787416617075602 \tInference time 4.4637062814500596e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.4089 - acc: 0.8737 - val_loss: 0.1854 - val_acc: 0.9414\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.1538 - acc: 0.9537 - val_loss: 0.0988 - val_acc: 0.9689\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1016 - acc: 0.9688 - val_loss: 0.0879 - val_acc: 0.9724\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0744 - acc: 0.9771 - val_loss: 0.0689 - val_acc: 0.9789\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0572 - acc: 0.9822 - val_loss: 0.0663 - val_acc: 0.9806\n",
      "Done inference\n",
      "\n",
      " Batch size: 600 \tTrain time: 0.00014832123120625814 \tInference time 4.675558143191867e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.4360 - acc: 0.8662 - val_loss: 0.2003 - val_acc: 0.9384\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1627 - acc: 0.9506 - val_loss: 0.1061 - val_acc: 0.9680\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1076 - acc: 0.9668 - val_loss: 0.0862 - val_acc: 0.9733\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0794 - acc: 0.9752 - val_loss: 0.0715 - val_acc: 0.9779\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0613 - acc: 0.9811 - val_loss: 0.0646 - val_acc: 0.9796\n",
      "Done inference\n",
      "\n",
      " Batch size: 700 \tTrain time: 0.00014903778076171876 \tInference time 4.4897629155053035e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.4602 - acc: 0.8575 - val_loss: 0.1934 - val_acc: 0.9402\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.1734 - acc: 0.9470 - val_loss: 0.1203 - val_acc: 0.9615\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.1174 - acc: 0.9646 - val_loss: 0.0910 - val_acc: 0.9713\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0856 - acc: 0.9728 - val_loss: 0.0878 - val_acc: 0.9736\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0674 - acc: 0.9785 - val_loss: 0.0738 - val_acc: 0.9768\n",
      "Done inference\n",
      "\n",
      " Batch size: 800 \tTrain time: 0.00014415589888890583 \tInference time 4.344169994195302e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.5026 - acc: 0.8428 - val_loss: 0.1983 - val_acc: 0.9398\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.1922 - acc: 0.9421 - val_loss: 0.1347 - val_acc: 0.9575\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.1319 - acc: 0.9595 - val_loss: 0.1015 - val_acc: 0.9699\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.0937 - acc: 0.9715 - val_loss: 0.0783 - val_acc: 0.9761\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0723 - acc: 0.9773 - val_loss: 0.0847 - val_acc: 0.9748\n",
      "Done inference\n",
      "\n",
      " Batch size: 900 \tTrain time: 0.00013813772281010946 \tInference time 4.3009133140246076e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.5143 - acc: 0.8385 - val_loss: 0.2104 - val_acc: 0.9359\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1991 - acc: 0.9400 - val_loss: 0.1282 - val_acc: 0.9603\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.1314 - acc: 0.9598 - val_loss: 0.1058 - val_acc: 0.9672\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0958 - acc: 0.9706 - val_loss: 0.0922 - val_acc: 0.9710\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0749 - acc: 0.9766 - val_loss: 0.0708 - val_acc: 0.9767\n",
      "Done inference\n",
      "\n",
      " Batch size: 1000 \tTrain time: 0.00013527613162994385 \tInference time 4.3336531519889835e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.7143 - acc: 0.7721 - val_loss: 0.3523 - val_acc: 0.8904\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.2963 - acc: 0.9099 - val_loss: 0.2117 - val_acc: 0.9353\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.2115 - acc: 0.9354 - val_loss: 0.2406 - val_acc: 0.9221\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.1676 - acc: 0.9482 - val_loss: 0.1136 - val_acc: 0.9646\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.1360 - acc: 0.9588 - val_loss: 0.1046 - val_acc: 0.9673\n",
      "Done inference\n",
      "\n",
      " Batch size: 2000 \tTrain time: 0.0001311449098587036 \tInference time 4.224711457888285e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.8555 - acc: 0.7361 - val_loss: 0.3158 - val_acc: 0.9133\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3469 - acc: 0.8945 - val_loss: 0.2423 - val_acc: 0.9260\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2612 - acc: 0.9195 - val_loss: 0.2210 - val_acc: 0.9316\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2125 - acc: 0.9355 - val_loss: 0.1869 - val_acc: 0.9400\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.1834 - acc: 0.9441 - val_loss: 0.1318 - val_acc: 0.9582\n",
      "Done inference\n",
      "\n",
      " Batch size: 3000 \tTrain time: 0.00012872459252675375 \tInference time 4.2954294880231225e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 1.0096 - acc: 0.6754 - val_loss: 0.4249 - val_acc: 0.8641\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.4189 - acc: 0.8702 - val_loss: 0.2728 - val_acc: 0.9221\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3082 - acc: 0.9046 - val_loss: 0.2228 - val_acc: 0.9341\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.2611 - acc: 0.9201 - val_loss: 0.1884 - val_acc: 0.9444\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.2018 - acc: 0.9398 - val_loss: 0.1652 - val_acc: 0.9493\n",
      "Done inference\n",
      "\n",
      " Batch size: 4000 \tTrain time: 0.00012759259462356569 \tInference time 4.31203564008077e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 1.0855 - acc: 0.6550 - val_loss: 0.4808 - val_acc: 0.8548\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.4399 - acc: 0.8668 - val_loss: 0.3364 - val_acc: 0.8970\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.3361 - acc: 0.8969 - val_loss: 0.2896 - val_acc: 0.9107\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.2700 - acc: 0.9189 - val_loss: 0.2636 - val_acc: 0.9163\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2473 - acc: 0.9251 - val_loss: 0.1976 - val_acc: 0.9377\n",
      "Done inference\n",
      "\n",
      " Batch size: 5000 \tTrain time: 0.00013218597412109376 \tInference time 4.0772426128387454e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 1.1870 - acc: 0.6168 - val_loss: 0.6084 - val_acc: 0.7915\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.5043 - acc: 0.8432 - val_loss: 0.3303 - val_acc: 0.9053\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.3846 - acc: 0.8821 - val_loss: 0.3412 - val_acc: 0.8972\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3151 - acc: 0.9051 - val_loss: 0.2219 - val_acc: 0.9341\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.2785 - acc: 0.9169 - val_loss: 0.2018 - val_acc: 0.9381\n",
      "Done inference\n",
      "\n",
      " Batch size: 6000 \tTrain time: 0.00013454809506734211 \tInference time 4.1637399792671206e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 1.2733 - acc: 0.5957 - val_loss: 0.6352 - val_acc: 0.7973\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.5451 - acc: 0.8335 - val_loss: 0.4743 - val_acc: 0.8368\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.3797 - acc: 0.8857 - val_loss: 0.3692 - val_acc: 0.8828\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.3475 - acc: 0.8942 - val_loss: 0.2421 - val_acc: 0.9278\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.3137 - acc: 0.9013 - val_loss: 0.2573 - val_acc: 0.9206\n",
      "Done inference\n",
      "\n",
      " Batch size: 7000 \tTrain time: 0.0001388829509417216 \tInference time 4.585602945751614e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 1.3319 - acc: 0.5706 - val_loss: 0.5605 - val_acc: 0.8516\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.6225 - acc: 0.8077 - val_loss: 0.3710 - val_acc: 0.8929\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.4017 - acc: 0.8797 - val_loss: 0.3124 - val_acc: 0.9086\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.3534 - acc: 0.8902 - val_loss: 0.3229 - val_acc: 0.9031\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3045 - acc: 0.9090 - val_loss: 0.2531 - val_acc: 0.9233\n",
      "Done inference\n",
      "\n",
      " Batch size: 8000 \tTrain time: 0.0001395345679918925 \tInference time 4.52654759089152e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 1.4310 - acc: 0.5292 - val_loss: 0.7192 - val_acc: 0.7554\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.6370 - acc: 0.7986 - val_loss: 0.4153 - val_acc: 0.8775\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.4417 - acc: 0.8651 - val_loss: 0.3279 - val_acc: 0.9025\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.3905 - acc: 0.8783 - val_loss: 0.2853 - val_acc: 0.9149\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.3333 - acc: 0.8987 - val_loss: 0.2656 - val_acc: 0.9230\n",
      "Done inference\n",
      "\n",
      " Batch size: 9000 \tTrain time: 0.00013844428618748982 \tInference time 4.459146062533061e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 1.4856 - acc: 0.5026 - val_loss: 0.8116 - val_acc: 0.7237\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.7005 - acc: 0.7847 - val_loss: 0.5053 - val_acc: 0.8349\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.4748 - acc: 0.8535 - val_loss: 0.4549 - val_acc: 0.8527\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.4118 - acc: 0.8721 - val_loss: 0.2990 - val_acc: 0.9144\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.3878 - acc: 0.8768 - val_loss: 0.2616 - val_acc: 0.9290\n",
      "Done inference\n",
      "\n",
      " Batch size: 10000 \tTrain time: 0.00013860533952713013 \tInference time 4.4684839248657224e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "TRAIN ON GPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.7819 - acc: 0.9175 - val_loss: 0.6079 - val_acc: 0.9441\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.6456 - acc: 0.9464 - val_loss: 0.3999 - val_acc: 0.9653\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.5608 - acc: 0.9554 - val_loss: 0.4589 - val_acc: 0.9639\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 244s 4ms/step - loss: 0.5203 - acc: 0.9589 - val_loss: 0.4350 - val_acc: 0.9659\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 247s 4ms/step - loss: 0.4815 - acc: 0.9627 - val_loss: 0.4455 - val_acc: 0.9670\n",
      "Done inference\n",
      "\n",
      " Batch size: 1 \tTrain time: 0.00404872907559077 \tInference time 0.0006078497568766276 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.5684 - acc: 0.9226 - val_loss: 0.5869 - val_acc: 0.9439\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.5580 - acc: 0.9487 - val_loss: 0.3719 - val_acc: 0.9651\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.5339 - acc: 0.9554 - val_loss: 0.4063 - val_acc: 0.9676\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.4919 - acc: 0.9608 - val_loss: 0.4616 - val_acc: 0.9650\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 127s 2ms/step - loss: 0.4644 - acc: 0.9636 - val_loss: 0.3898 - val_acc: 0.9706\n",
      "Done inference\n",
      "\n",
      " Batch size: 2 \tTrain time: 0.0021736720434824626 \tInference time 0.00029731813669204714 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.4818 - acc: 0.9235 - val_loss: 0.4250 - val_acc: 0.9509\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.4963 - acc: 0.9503 - val_loss: 0.4188 - val_acc: 0.9628\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.4731 - acc: 0.9569 - val_loss: 0.3954 - val_acc: 0.9673\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.4705 - acc: 0.9610 - val_loss: 0.3954 - val_acc: 0.9670\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.4379 - acc: 0.9643 - val_loss: 0.3835 - val_acc: 0.9701\n",
      "Done inference\n",
      "\n",
      " Batch size: 3 \tTrain time: 0.0013738627592722575 \tInference time 0.00020821809371312459 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.4223 - acc: 0.9253 - val_loss: 0.3628 - val_acc: 0.9527\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.4260 - acc: 0.9518 - val_loss: 0.3820 - val_acc: 0.9549\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.4531 - acc: 0.9571 - val_loss: 0.3382 - val_acc: 0.9714\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 65s 1ms/step - loss: 0.4307 - acc: 0.9624 - val_loss: 0.3233 - val_acc: 0.9707\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.3868 - acc: 0.9667 - val_loss: 0.3373 - val_acc: 0.9718\n",
      "Done inference\n",
      "\n",
      " Batch size: 4 \tTrain time: 0.0010584680398305257 \tInference time 0.00015274801850318907 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 51s 846us/step - loss: 0.3710 - acc: 0.9276 - val_loss: 0.3158 - val_acc: 0.9555\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 51s 843us/step - loss: 0.4034 - acc: 0.9528 - val_loss: 0.2945 - val_acc: 0.9677\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 52s 866us/step - loss: 0.3877 - acc: 0.9590 - val_loss: 0.3319 - val_acc: 0.9654\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 50s 840us/step - loss: 0.3955 - acc: 0.9630 - val_loss: 0.4109 - val_acc: 0.9647\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 51s 844us/step - loss: 0.3917 - acc: 0.9646 - val_loss: 0.2974 - val_acc: 0.9708\n",
      "Done inference\n",
      "\n",
      " Batch size: 5 \tTrain time: 0.0008494894766807556 \tInference time 0.00012337908744812013 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 41s 687us/step - loss: 0.3519 - acc: 0.9273 - val_loss: 0.2103 - val_acc: 0.9629\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 42s 697us/step - loss: 0.3577 - acc: 0.9528 - val_loss: 0.2547 - val_acc: 0.9648\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 42s 695us/step - loss: 0.3855 - acc: 0.9573 - val_loss: 0.2728 - val_acc: 0.9675\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 44s 741us/step - loss: 0.3728 - acc: 0.9615 - val_loss: 0.3603 - val_acc: 0.9637\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 43s 712us/step - loss: 0.3809 - acc: 0.9641 - val_loss: 0.4216 - val_acc: 0.9642\n",
      "Done inference\n",
      "\n",
      " Batch size: 6 \tTrain time: 0.0007073731883366903 \tInference time 0.00010258510112762451 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 37s 611us/step - loss: 0.3240 - acc: 0.9308 - val_loss: 0.2793 - val_acc: 0.9542\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 36s 602us/step - loss: 0.3267 - acc: 0.9529 - val_loss: 0.2694 - val_acc: 0.9624\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 38s 626us/step - loss: 0.3355 - acc: 0.9596 - val_loss: 0.2588 - val_acc: 0.9663\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 36s 598us/step - loss: 0.3437 - acc: 0.9632 - val_loss: 0.3186 - val_acc: 0.9672\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 37s 612us/step - loss: 0.3509 - acc: 0.9656 - val_loss: 0.3094 - val_acc: 0.9689\n",
      "Done inference\n",
      "\n",
      " Batch size: 7 \tTrain time: 0.000611465265750885 \tInference time 9.054846366246541e-05 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 33s 548us/step - loss: 0.3022 - acc: 0.9324 - val_loss: 0.2265 - val_acc: 0.9610\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 32s 536us/step - loss: 0.3005 - acc: 0.9540 - val_loss: 0.2412 - val_acc: 0.9648\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 31s 521us/step - loss: 0.3137 - acc: 0.9589 - val_loss: 0.3693 - val_acc: 0.9557\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 30s 507us/step - loss: 0.3196 - acc: 0.9641 - val_loss: 0.3337 - val_acc: 0.9673\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 31s 520us/step - loss: 0.3298 - acc: 0.9660 - val_loss: 0.2583 - val_acc: 0.9738\n",
      "Done inference\n",
      "\n",
      " Batch size: 8 \tTrain time: 0.0005274283957481384 \tInference time 7.262385686238607e-05 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 25s 423us/step - loss: 0.3017 - acc: 0.9303 - val_loss: 0.2233 - val_acc: 0.9580\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 25s 423us/step - loss: 0.2825 - acc: 0.9567 - val_loss: 0.1990 - val_acc: 0.9676\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 25s 412us/step - loss: 0.2968 - acc: 0.9603 - val_loss: 0.2509 - val_acc: 0.9708\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 26s 431us/step - loss: 0.3101 - acc: 0.9639 - val_loss: 0.3651 - val_acc: 0.9592\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 25s 418us/step - loss: 0.3125 - acc: 0.9661 - val_loss: 0.2603 - val_acc: 0.9729\n",
      "Done inference\n",
      "\n",
      " Batch size: 9 \tTrain time: 0.0004222261953353882 \tInference time 6.633789274427626e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 23s 390us/step - loss: 0.2822 - acc: 0.9331 - val_loss: 0.2528 - val_acc: 0.9573\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 24s 398us/step - loss: 0.2690 - acc: 0.9574 - val_loss: 0.1765 - val_acc: 0.9720\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 23s 391us/step - loss: 0.2734 - acc: 0.9613 - val_loss: 0.2900 - val_acc: 0.9657\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 23s 384us/step - loss: 0.2859 - acc: 0.9649 - val_loss: 0.3123 - val_acc: 0.9673\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 23s 382us/step - loss: 0.2827 - acc: 0.9675 - val_loss: 0.2436 - val_acc: 0.9711\n",
      "Done inference\n",
      "\n",
      " Batch size: 10 \tTrain time: 0.0003895996753374736 \tInference time 5.8977276749081084e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.2335 - acc: 0.9352 - val_loss: 0.1398 - val_acc: 0.9652\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.1645 - acc: 0.9644 - val_loss: 0.1494 - val_acc: 0.9707\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.1609 - acc: 0.9687 - val_loss: 0.1535 - val_acc: 0.9737\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.1646 - acc: 0.9713 - val_loss: 0.1741 - val_acc: 0.9712\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.1741 - acc: 0.9725 - val_loss: 0.1722 - val_acc: 0.9753\n",
      "Done inference\n",
      "\n",
      " Batch size: 20 \tTrain time: 0.00019369928201039632 \tInference time 3.504659632841746e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.2230 - acc: 0.9354 - val_loss: 0.1229 - val_acc: 0.9676\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.1349 - acc: 0.9672 - val_loss: 0.1214 - val_acc: 0.9702\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.1241 - acc: 0.9716 - val_loss: 0.1380 - val_acc: 0.9739\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.1210 - acc: 0.9753 - val_loss: 0.1413 - val_acc: 0.9750\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.1256 - acc: 0.9765 - val_loss: 0.1286 - val_acc: 0.9754\n",
      "Done inference\n",
      "\n",
      " Batch size: 30 \tTrain time: 0.0001330933650334676 \tInference time 2.5301330884297687e-05 (5)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2202 - acc: 0.9343 - val_loss: 0.1000 - val_acc: 0.9707\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.1182 - acc: 0.9683 - val_loss: 0.1084 - val_acc: 0.9735\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.1065 - acc: 0.9735 - val_loss: 0.1140 - val_acc: 0.9724\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1008 - acc: 0.9776 - val_loss: 0.1338 - val_acc: 0.9748\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0989 - acc: 0.9784 - val_loss: 0.1072 - val_acc: 0.9801\n",
      "Done inference\n",
      "\n",
      " Batch size: 40 \tTrain time: 0.00010101508696873982 \tInference time 1.8209154076046414e-05 (6)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2217 - acc: 0.9334 - val_loss: 0.1190 - val_acc: 0.9655\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1117 - acc: 0.9684 - val_loss: 0.1116 - val_acc: 0.9693\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0976 - acc: 0.9758 - val_loss: 0.0920 - val_acc: 0.9778\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0841 - acc: 0.9790 - val_loss: 0.0956 - val_acc: 0.9798\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0844 - acc: 0.9811 - val_loss: 0.1028 - val_acc: 0.9801\n",
      "Done inference\n",
      "\n",
      " Batch size: 50 \tTrain time: 8.024230639139811e-05 \tInference time 1.5557470208122618e-05 (7)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2211 - acc: 0.9323 - val_loss: 0.1114 - val_acc: 0.9677\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1083 - acc: 0.9692 - val_loss: 0.0903 - val_acc: 0.9750\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0860 - acc: 0.9760 - val_loss: 0.0898 - val_acc: 0.9778\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0786 - acc: 0.9794 - val_loss: 0.0861 - val_acc: 0.9787\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0717 - acc: 0.9818 - val_loss: 0.0995 - val_acc: 0.9790\n",
      "Done inference\n",
      "\n",
      " Batch size: 60 \tTrain time: 6.55035392443339e-05 \tInference time 1.2837229172388712e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2260 - acc: 0.9318 - val_loss: 0.1346 - val_acc: 0.9617\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 58us/step - loss: 0.1068 - acc: 0.9681 - val_loss: 0.1007 - val_acc: 0.9687\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0832 - acc: 0.9772 - val_loss: 0.0914 - val_acc: 0.9759\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0723 - acc: 0.9807 - val_loss: 0.0846 - val_acc: 0.9783\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0612 - acc: 0.9833 - val_loss: 0.1051 - val_acc: 0.9779\n",
      "Done inference\n",
      "\n",
      " Batch size: 70 \tTrain time: 5.7511791388193766e-05 \tInference time 1.172607938448588e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.2260 - acc: 0.9314 - val_loss: 0.1030 - val_acc: 0.9681\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.1018 - acc: 0.9696 - val_loss: 0.0903 - val_acc: 0.9729\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0802 - acc: 0.9769 - val_loss: 0.0842 - val_acc: 0.9764\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0664 - acc: 0.9808 - val_loss: 0.0837 - val_acc: 0.9802\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0596 - acc: 0.9839 - val_loss: 0.0744 - val_acc: 0.9839\n",
      "Done inference\n",
      "\n",
      " Batch size: 80 \tTrain time: 5.007208585739136e-05 \tInference time 1.1071547865867615e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2351 - acc: 0.9279 - val_loss: 0.1015 - val_acc: 0.9688\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1039 - acc: 0.9690 - val_loss: 0.0885 - val_acc: 0.9733\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0809 - acc: 0.9768 - val_loss: 0.0803 - val_acc: 0.9774\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0656 - acc: 0.9815 - val_loss: 0.0784 - val_acc: 0.9791\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0546 - acc: 0.9841 - val_loss: 0.0858 - val_acc: 0.9797\n",
      "Done inference\n",
      "\n",
      " Batch size: 90 \tTrain time: 4.683149814605713e-05 \tInference time 1.0424550374348959e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2358 - acc: 0.9281 - val_loss: 0.1163 - val_acc: 0.9635\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1025 - acc: 0.9690 - val_loss: 0.0720 - val_acc: 0.9779\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0761 - acc: 0.9776 - val_loss: 0.0838 - val_acc: 0.9770\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0637 - acc: 0.9812 - val_loss: 0.0807 - val_acc: 0.9786\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0566 - acc: 0.9844 - val_loss: 0.0704 - val_acc: 0.9827\n",
      "Done inference\n",
      "\n",
      " Batch size: 100 \tTrain time: 3.942339022954305e-05 \tInference time 9.120073583390978e-06 (9)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.2748 - acc: 0.9158 - val_loss: 0.1169 - val_acc: 0.9622\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1074 - acc: 0.9676 - val_loss: 0.1077 - val_acc: 0.9671\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0762 - acc: 0.9767 - val_loss: 0.0778 - val_acc: 0.9766\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0589 - acc: 0.9819 - val_loss: 0.0730 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0466 - acc: 0.9853 - val_loss: 0.0718 - val_acc: 0.9803\n",
      "Done inference\n",
      "\n",
      " Batch size: 200 \tTrain time: 2.2523438930511476e-05 \tInference time 5.927848093437426e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3105 - acc: 0.9029 - val_loss: 0.1378 - val_acc: 0.9563\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1150 - acc: 0.9646 - val_loss: 0.0841 - val_acc: 0.9732\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0771 - acc: 0.9761 - val_loss: 0.0761 - val_acc: 0.9772\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0601 - acc: 0.9808 - val_loss: 0.0685 - val_acc: 0.9783\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0467 - acc: 0.9850 - val_loss: 0.0658 - val_acc: 0.9796\n",
      "Done inference\n",
      "\n",
      " Batch size: 300 \tTrain time: 1.637341578801473e-05 \tInference time 4.755939498092189e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3409 - acc: 0.8958 - val_loss: 0.1574 - val_acc: 0.9504\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.1281 - acc: 0.9607 - val_loss: 0.0894 - val_acc: 0.9732\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0845 - acc: 0.9735 - val_loss: 0.0767 - val_acc: 0.9768\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0638 - acc: 0.9801 - val_loss: 0.0783 - val_acc: 0.9770\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0496 - acc: 0.9841 - val_loss: 0.0717 - val_acc: 0.9788\n",
      "Done inference\n",
      "\n",
      " Batch size: 400 \tTrain time: 1.3467741012573242e-05 \tInference time 4.496771277803363e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3745 - acc: 0.8849 - val_loss: 0.1418 - val_acc: 0.9566\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.1370 - acc: 0.9581 - val_loss: 0.0963 - val_acc: 0.9687\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0923 - acc: 0.9712 - val_loss: 0.0754 - val_acc: 0.9763\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0668 - acc: 0.9790 - val_loss: 0.0738 - val_acc: 0.9762\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0540 - acc: 0.9830 - val_loss: 0.0706 - val_acc: 0.9790\n",
      "Done inference\n",
      "\n",
      " Batch size: 500 \tTrain time: 1.2243537108103434e-05 \tInference time 4.051435354984168e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.4111 - acc: 0.8732 - val_loss: 0.1499 - val_acc: 0.9533\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1532 - acc: 0.9532 - val_loss: 0.1140 - val_acc: 0.9629\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1021 - acc: 0.9686 - val_loss: 0.0866 - val_acc: 0.9726\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0740 - acc: 0.9770 - val_loss: 0.0881 - val_acc: 0.9734\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0579 - acc: 0.9818 - val_loss: 0.0662 - val_acc: 0.9797\n",
      "Done inference\n",
      "\n",
      " Batch size: 600 \tTrain time: 1.0306185881296794e-05 \tInference time 3.6934071116977266e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.4263 - acc: 0.8670 - val_loss: 0.1616 - val_acc: 0.9517\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1615 - acc: 0.9511 - val_loss: 0.1151 - val_acc: 0.9644\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1062 - acc: 0.9672 - val_loss: 0.0886 - val_acc: 0.9727\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0786 - acc: 0.9756 - val_loss: 0.0966 - val_acc: 0.9684\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0612 - acc: 0.9814 - val_loss: 0.0793 - val_acc: 0.9754\n",
      "Done inference\n",
      "\n",
      " Batch size: 700 \tTrain time: 9.374789396921794e-06 \tInference time 3.716956575711568e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.4581 - acc: 0.8564 - val_loss: 0.1831 - val_acc: 0.9447\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1761 - acc: 0.9453 - val_loss: 0.1199 - val_acc: 0.9606\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1168 - acc: 0.9640 - val_loss: 0.0953 - val_acc: 0.9714\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0871 - acc: 0.9726 - val_loss: 0.0752 - val_acc: 0.9764\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0661 - acc: 0.9785 - val_loss: 0.0704 - val_acc: 0.9783\n",
      "Done inference\n",
      "\n",
      " Batch size: 800 \tTrain time: 8.592690626780192e-06 \tInference time 3.686127879402854e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.4876 - acc: 0.8482 - val_loss: 0.2061 - val_acc: 0.9357\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1870 - acc: 0.9434 - val_loss: 0.1320 - val_acc: 0.9611\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1265 - acc: 0.9617 - val_loss: 0.0927 - val_acc: 0.9719\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0939 - acc: 0.9706 - val_loss: 0.0869 - val_acc: 0.9728\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0695 - acc: 0.9783 - val_loss: 0.0760 - val_acc: 0.9763\n",
      "Done inference\n",
      "\n",
      " Batch size: 900 \tTrain time: 8.687284787495932e-06 \tInference time 3.4788896640141806e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.5116 - acc: 0.8405 - val_loss: 0.2362 - val_acc: 0.9277\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1997 - acc: 0.9392 - val_loss: 0.1496 - val_acc: 0.9534\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1356 - acc: 0.9576 - val_loss: 0.1125 - val_acc: 0.9630\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0981 - acc: 0.9702 - val_loss: 0.0995 - val_acc: 0.9682\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0772 - acc: 0.9758 - val_loss: 0.0757 - val_acc: 0.9757\n",
      "Done inference\n",
      "\n",
      " Batch size: 1000 \tTrain time: 8.486604690551757e-06 \tInference time 3.5086335557879825e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.7036 - acc: 0.7816 - val_loss: 0.3302 - val_acc: 0.8908\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2876 - acc: 0.9126 - val_loss: 0.2980 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2000 - acc: 0.9398 - val_loss: 0.2076 - val_acc: 0.9315\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.1577 - acc: 0.9516 - val_loss: 0.1233 - val_acc: 0.9602\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.1315 - acc: 0.9596 - val_loss: 0.1260 - val_acc: 0.9586\n",
      "Done inference\n",
      "\n",
      " Batch size: 2000 \tTrain time: 7.317152023315429e-06 \tInference time 3.765739334954156e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.8466 - acc: 0.7300 - val_loss: 0.3930 - val_acc: 0.8830\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3435 - acc: 0.8975 - val_loss: 0.2652 - val_acc: 0.9199\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2703 - acc: 0.9163 - val_loss: 0.1941 - val_acc: 0.9401\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2158 - acc: 0.9340 - val_loss: 0.2238 - val_acc: 0.9297\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.1697 - acc: 0.9492 - val_loss: 0.1450 - val_acc: 0.9548\n",
      "Done inference\n",
      "\n",
      " Batch size: 3000 \tTrain time: 5.612080097198486e-06 \tInference time 4.391465403816917e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.9745 - acc: 0.6910 - val_loss: 0.4635 - val_acc: 0.8470\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.4227 - acc: 0.8718 - val_loss: 0.2615 - val_acc: 0.9240\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3189 - acc: 0.9014 - val_loss: 0.2203 - val_acc: 0.9356\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2468 - acc: 0.9272 - val_loss: 0.2240 - val_acc: 0.9294\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2121 - acc: 0.9345 - val_loss: 0.1465 - val_acc: 0.9550\n",
      "Done inference\n",
      "\n",
      " Batch size: 4000 \tTrain time: 6.669041315714518e-06 \tInference time 3.914385881179419e-06 (13)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.0936 - acc: 0.6486 - val_loss: 0.6084 - val_acc: 0.7882\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4623 - acc: 0.8571 - val_loss: 0.3067 - val_acc: 0.9123\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3167 - acc: 0.9052 - val_loss: 0.2845 - val_acc: 0.9079\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3023 - acc: 0.9049 - val_loss: 0.2157 - val_acc: 0.9336\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2455 - acc: 0.9250 - val_loss: 0.2000 - val_acc: 0.9353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done inference\n",
      "\n",
      " Batch size: 5000 \tTrain time: 5.650022029876709e-06 \tInference time 3.910848727593055e-06 (13)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 1.1729 - acc: 0.6287 - val_loss: 0.6625 - val_acc: 0.7830\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.5264 - acc: 0.8339 - val_loss: 0.3723 - val_acc: 0.8885\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3565 - acc: 0.8940 - val_loss: 0.3078 - val_acc: 0.9038\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3281 - acc: 0.8979 - val_loss: 0.2165 - val_acc: 0.9372\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2786 - acc: 0.9171 - val_loss: 0.2065 - val_acc: 0.9374\n",
      "Done inference\n",
      "\n",
      " Batch size: 6000 \tTrain time: 6.9256997108459475e-06 \tInference time 4.416551854875353e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 1.2761 - acc: 0.5930 - val_loss: 0.5852 - val_acc: 0.7982\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.5472 - acc: 0.8273 - val_loss: 0.4285 - val_acc: 0.8706\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3944 - acc: 0.8812 - val_loss: 0.3025 - val_acc: 0.9095\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3341 - acc: 0.8966 - val_loss: 0.2483 - val_acc: 0.9261\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2861 - acc: 0.9155 - val_loss: 0.2469 - val_acc: 0.9246\n",
      "Done inference\n",
      "\n",
      " Batch size: 7000 \tTrain time: 6.716055075327555e-06 \tInference time 7.853390552379466e-06 (9)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.3832 - acc: 0.5557 - val_loss: 0.6437 - val_acc: 0.7868\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.6143 - acc: 0.8048 - val_loss: 0.5162 - val_acc: 0.8249\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4180 - acc: 0.8758 - val_loss: 0.3693 - val_acc: 0.8876\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3398 - acc: 0.8991 - val_loss: 0.3011 - val_acc: 0.9083\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3402 - acc: 0.8956 - val_loss: 0.2289 - val_acc: 0.9322\n",
      "Done inference\n",
      "\n",
      " Batch size: 8000 \tTrain time: 5.322608153025309e-06 \tInference time 6.637176239129269e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4146 - acc: 0.5482 - val_loss: 0.5582 - val_acc: 0.8554\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.6527 - acc: 0.7999 - val_loss: 0.3618 - val_acc: 0.9075\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4730 - acc: 0.8505 - val_loss: 0.4235 - val_acc: 0.8763\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3325 - acc: 0.9069 - val_loss: 0.2520 - val_acc: 0.9280\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3868 - acc: 0.8815 - val_loss: 0.2477 - val_acc: 0.9285\n",
      "Done inference\n",
      "\n",
      " Batch size: 9000 \tTrain time: 5.2799081802368165e-06 \tInference time 7.030390898386637e-06 (10)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4886 - acc: 0.5150 - val_loss: 0.7202 - val_acc: 0.7908\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 3us/step - loss: 0.7236 - acc: 0.7743 - val_loss: 0.4806 - val_acc: 0.8539\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 3us/step - loss: 0.4983 - acc: 0.8444 - val_loss: 0.3339 - val_acc: 0.9055\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4044 - acc: 0.8757 - val_loss: 0.4669 - val_acc: 0.8374\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3529 - acc: 0.8916 - val_loss: 0.2945 - val_acc: 0.9137\n",
      "Done inference\n",
      "\n",
      " Batch size: 10000 \tTrain time: 5.695474942525228e-06 \tInference time 7.02138622601827e-06 (10)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST MLP\")\n",
    "print()\n",
    "mlp_cpu = MNIST_MLP('/cpu:0')\n",
    "mlp_gpu = MNIST_MLP('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally, the CNN implementation of MNIST will run out of memory. So we will create a backup now of train time and inference times for MLP MNIST in case this happens and we are forced to restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = open(\"backup.txt\", 'w')\n",
    "\n",
    "assert len(mlp_cpu.batch_sizes) == len(mlp_gpu.batch_sizes) == len(mlp_cpu.train_times) == len(mlp_cpu.inference_times) \\\n",
    "                 == len(mlp_gpu.train_times) == len(mlp_gpu.inference_times)\n",
    "for i in range(len(mlp_cpu.batch_sizes)):\n",
    "    assert mlp_cpu.batch_sizes[i] == mlp_gpu.batch_sizes[i]\n",
    "    backup.write(str(mlp_cpu.batch_sizes[i]) + '|' +\n",
    "                 str(mlp_cpu.train_times[i]) + '|' +\n",
    "                 str(mlp_cpu.inference_times[i]) + '|' +\n",
    "                 str(mlp_gpu.train_times[i]) + '|' +\n",
    "                 str(mlp_gpu.inference_times[i]) + '|' + '\\n')\n",
    "\n",
    "backup.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to get the data for the CNN implementations of MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_CNN\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-21a2aeb7a718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcnn_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f6f2c9eb7b2d>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0minference_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Batch size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\tTrain time:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\tInference time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(%s)'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0minference_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f6f2c9eb7b2d>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         history = self.model.fit(self.x_train, self.y_train, batch_size=batch_size, epochs=NUM_EPOCHS, verbose=1,\n\u001b[0;32m---> 44\u001b[0;31m                             validation_data=(self.x_test, self.y_test))\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;31m# update accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mnew_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;31m# use the new accumulator and the *old* delta_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(x, new_x)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \"\"\"\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    221\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    222\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    224\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     62\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                   use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_Flatten\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;34m\"\"\"Converts [1, 2, [3, 4], [5]] to [1, 2, 3, 4, 5].\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;31m# [1, 2, [3, 4], [5]] -> [[1], [2], [3, 4], [5]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0ml_of_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   \u001b[0;31m# [[1], [2], [3, 4], [5]] -> [1, 2, 3, 4, 5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_of_l\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;34m\"\"\"Converts [1, 2, [3, 4], [5]] to [1, 2, 3, 4, 5].\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;31m# [1, 2, [3, 4], [5]] -> [[1], [2], [3, 4], [5]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0ml_of_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   \u001b[0;31m# [[1], [2], [3, 4], [5]] -> [1, 2, 3, 4, 5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_of_l\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_IsListValue\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"MNIST_CNN\")\n",
    "cnn_cpu = MNIST_CNN('/cpu:0')\n",
    "cnn_gpu = MNIST_CNN('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"MLPdata.txt\", 'r')\n",
    "lines = f.read().split('\\n')\n",
    "f.close()\n",
    "mlp_cpu = MNIST_MLP(\"\\cpu:0\")\n",
    "mlp_gpu = MNIST_MLP(\"\\gpu:0\")\n",
    "\n",
    "first = False\n",
    "for line in lines:\n",
    "    if line == '': continue\n",
    "    batch_size, train_time, inference_time = line.split('|')\n",
    "    if batch_size == '1':\n",
    "        first = not first\n",
    "    if first:\n",
    "        mlp_cpu.batch_sizes.append(int(batch_size))\n",
    "        mlp_cpu.train_times.append(float(train_time))\n",
    "        mlp_cpu.inference_times.append(float(inference_time))\n",
    "    else:\n",
    "        mlp_gpu.batch_sizes.append(int(batch_size))\n",
    "        mlp_gpu.train_times.append(float(train_time))\n",
    "        mlp_gpu.inference_times.append(float(inference_time))\n",
    "        \n",
    "    \n",
    "cnn_cpu = MNIST_CNN(\"\\cpu:0\")\n",
    "f = open(\"CNNdata.txt\", 'r')\n",
    "lines = f.read().split('\\n')\n",
    "f.close()\n",
    "for line in lines:\n",
    "    if line == '': continue\n",
    "    batch_size, train_time, inference_time = line.split('|')\n",
    "    cnn_cpu.batch_sizes.append(int(batch_size))\n",
    "    cnn_cpu.train_times.append(float(train_time))\n",
    "    cnn_cpu.inference_times.append(float(inference_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Data\n",
    "Now we wish to compare train time and inference time between CPUs and GPUs. We will make two plots, one for train time and one for inference time. First, we import the required modules. Each plot will have both MLP and CNN on it, and they can be compared or viewed as separate. First we import the required modules.\n",
    "\n",
    "In general, MLP is plotted with cool colors and CNN is plotted with warm colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_mlp = np.array(mlp_cpu.batch_sizes)\n",
    "x_cnn = np.array(cnn_cpu.batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the data we have gathered and obtain a graph of train times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_mlp, mlp_cpu.train_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_mlp, mlp_gpu.train_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x_cnn, cnn_cpu.train_times, c='y', alpha = 0.5, marker='^')\n",
    "#plt.scatter(x_cnn, cnn_gpu.train_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.000001, 0.02])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a graph of inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_mlp, mlp_cpu.inference_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_mlp, mlp_gpu.inference_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x_cnn, cnn_cpu.inference_times, c='y', alpha = 0.5, marker='^')\n",
    "#plt.scatter(x_cnn, cnn_gpu.inference_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.000001, 0.003])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a couple other plots, such as the performance gain in train time and inference time in using GPUs over CPUs. This is train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_improvement(cpu_times, gpu_times):\n",
    "    gain = []\n",
    "    for i in range(len(cpu_times)):\n",
    "        gain.append(cpu_times[i] / gpu_times[i] * 100)\n",
    "    return np.array(gain)\n",
    "\n",
    "gain_train_mlp = get_improvement(mlp_cpu.train_times, mlp_gpu.train_times)\n",
    "#gain_train_cnn = get_improvement(cnn_cpu.train_times, cnn_gpu.train_times)\n",
    "\n",
    "plt.scatter(x_mlp, gain_train_mlp, c='k', alpha = 0.5, marker = 'd')\n",
    "#plt.scatter(x_cnn, gain_train_cnn, c='c', alpha = 0.5, marker = '>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gain_inference_mlp = get_improvement(mlp_cpu.inference_times, mlp_gpu.inference_times)\n",
    "#gain_inference_cnn = get_improvement(cnn_cpu.inference_times, cnn_gpu.inference_times)\n",
    "\n",
    "plt.scatter(x_mlp, gain_inference_mlp, c='k', alpha = 0.5, marker='d')\n",
    "#plt.scatter(x_cnn, gain_inference_cnn, c='c', alpha = 0.5, marker='>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of the above data concludes this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
