{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the Train time and Inference Time of MNIST MLP and \n",
    "# CNN on CPUs and GPUs with `keras`\n",
    "\n",
    "The purpose of this notebook is to determine the relative speeds of training MNIST on CPUs and GPUs, as well as the relative speeds of inference. We will do this both for Convolutional Neural Network (CNN) and for Multi-Layer Perceptron (MLP) implementations of MNIST. It is similar to one of the experiments described in [this paper](https://arxiv.org/pdf/1904.08986.pdf) (arXiv:1904.08986v1 \\[physics.data-an\\]). The next step in this process will be to transform the code in this notebook into bare `tensorflow` code and compare the runtime between that and this `keras` implementation, and again between CPUs and GPUs in bare `tensorflow`. Then we will set the GPU implementation of MNIST up as a service. Ultimately, we look forward to running MNIST on TPUs and comparing runtime again.\n",
    "\n",
    "The code for the implementations of mnist using MLP with `keras` were pulled from [this github](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py), and the code for the CNN impementation was pulled from [this file](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) on the same github. I significantly changed both so that they would be more comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We import all the necessary classes and set some of the globals for the program. `NUM_CLASSES` is the number of categories to train mnist on. `NUM_EPOCHS` describes the number of epochs to run training over. `IMG_EDGE` is the side length of one of the (square) images, making the total pixel cound `IMG_EDGE ** 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adadelta\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Disable depreciation warnings\n",
    "try:\n",
    "    from tensorflow.python.util import deprecation\n",
    "    deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "except AttributeError:\n",
    "    print(\"Import failed\")\n",
    "\n",
    "# Set global constants\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 5\n",
    "IMG_EDGE = 28\n",
    "INFERENCE_TIME_THRESHOLD=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating MNIST\n",
    "\n",
    "Having imported all the necessary modules and defined the constants, we need to implement MNIST in `keras`. Since we plan to implement MNIST with MLP and with CNN, we will first create a parent class holding the common functions of both. Later, we will define two subclasses, one for MLP and once for CNN. The parent class has several functions:\n",
    "- `_load` and `_finish_load` load the default `keras` MNIST dataset in whatever form MLP or CNN wants the data to be in.\n",
    "- `_get_batch_sizes` sets a list of all the batch sizes to test. Because CNN uses so much memory, we will need to test smaller batch sizes or else the machine will crash. However, for the MLP implementation, we can train on very high batch sizes. Hence the batch sizes will be different for each case.\n",
    "- `_create` creates the MNIST model with a specific batch size.\n",
    "- `_load_inferences` loads several randomly generated images to be inferred on.\n",
    "- `_train` trains the MNIST model, keeping track of the time it takes to do so, and returns that time. The time it returns is actually the time to train _per iteration_, where one iteration is the number of epochs times the number of data points divided by the batch size.\n",
    "- `_predict` runs a number of inferences equal to the batch size the model was trained on and returns the time per inference. It does this multiple times to reduce uncertainty.\n",
    "- `get_data` runs all of the above functions in order to get the train times and inference times for the given machine type and implementation (MLP or CNN) for all the batch sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, machine):\n",
    "        self.machine = machine\n",
    "        self.model = None\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
    "        self.start_power = 0\n",
    "        self.end_power = 0\n",
    "        self.train_times = []\n",
    "        self.inference_times = []\n",
    "        \n",
    "    def _load(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _load_inferences(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _get_batch_sizes(self):\n",
    "        self.batch_sizes = []\n",
    "        for i in range(self.start_power, self.end_power):\n",
    "            self.batch_sizes += list(range(10**i, 10**(i+1), 10**i))\n",
    "        self.batch_sizes += [10**self.end_power]\n",
    "    \n",
    "    def _finish_load(self):\n",
    "        self.x_train = self.x_train.astype('float32')\n",
    "        self.x_test = self.x_test.astype('float32')\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        print('Train dataset size:', self.x_train.shape[0])\n",
    "        print('Test dataset size:', self.x_test.shape[0])\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = to_categorical(self.y_train, NUM_CLASSES)\n",
    "        self.y_test = to_categorical(self.y_test, NUM_CLASSES)\n",
    "    \n",
    "    def _create(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _train(self, batch_size):\n",
    "        start_time = time()\n",
    "        history = self.model.fit(self.x_train, self.y_train, batch_size=batch_size, epochs=NUM_EPOCHS, verbose=1,\n",
    "                            validation_data=(self.x_test, self.y_test))\n",
    "        end_time = time()\n",
    "        train_time = (end_time - start_time) / (NUM_EPOCHS * self.x_train.shape[0])\n",
    "\n",
    "        #loss, accuracy = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        return train_time\n",
    "    \n",
    "    def _predict(self, batch_size):\n",
    "        inference_time = 0\n",
    "        start_inference = time()\n",
    "        inference_num = 0\n",
    "        while True: # Do multiple trials\n",
    "            inputs = self._load_inferences()\n",
    "\n",
    "            start_time = time()\n",
    "            self.model.predict(inputs, batch_size=batch_size)\n",
    "            end_time = time()\n",
    "            \n",
    "            inference_time += end_time - start_time\n",
    "            inference_num += 1\n",
    "            if end_time - start_inference > INFERENCE_TIME_THRESHOLD:\n",
    "                print(\"Done inference\")\n",
    "                break\n",
    "        return inference_time / (inference_num * self.x_train.shape[0]), inference_num\n",
    "    \n",
    "    def get_data(self):\n",
    "        self.max_train = 0\n",
    "        self.max_inference = 0\n",
    "        \n",
    "        self._load()\n",
    "        self._get_batch_sizes()\n",
    "        for batch_size in self.batch_sizes:\n",
    "            self._create()\n",
    "            \n",
    "            train_time = self._train(batch_size)\n",
    "            inference_time, inference_num = self._predict(batch_size)\n",
    "            print('\\n','Batch size:', batch_size, '\\tTrain time:', train_time, '\\tInference time', inference_time, '(%s)'%inference_num)\n",
    "            print('+'*100)\n",
    "            self.train_times.append(train_time)\n",
    "            self.inference_times.append(inference_time)\n",
    "            \n",
    "            K.clear_session()# Clean up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our superclass, we may create two subclasses, one for MLP and one for CNN. They each handle the data and create the model differently, but in all other respects, `keras` allows us to treat them similarly, hence the superclass functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLP(MNIST):\n",
    "    def _load(self):\n",
    "        self.x_train = self.x_train.reshape(60000, IMG_EDGE**2)\n",
    "        self.x_test = self.x_test.reshape(10000, IMG_EDGE**2)\n",
    "        \n",
    "        self.start_power = 0\n",
    "        self.end_power = 4\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "    def _load_inferences(self):\n",
    "        return np.random.rand(self.x_train.shape[0], IMG_EDGE**2)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(512, activation='relu'))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=RMSprop(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(MNIST):\n",
    "    def _load(self):\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], 1, IMG_EDGE, IMG_EDGE)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], 1, IMG_EDGE, IMG_EDGE)\n",
    "            self.input_shape = (1, IMG_EDGE, IMG_EDGE)\n",
    "        else:\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "            self.input_shape = (IMG_EDGE, IMG_EDGE, 1)\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "        self.start_power = 0\n",
    "        self.end_power = 2# Smaller because CNN takes up more memory\n",
    "        \n",
    "    def _load_inferences(self):\n",
    "        return np.random.rand(self.x_train.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "            self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(128, activation='relu'))\n",
    "            self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=Adadelta(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Now that we have defined all the methods we need to gather data on the train time and inference time of MNIST on different machines with different implementations, all we need to do is call the functions. `get_data` will generate our lists of train times and inference times for every batch size. This will take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST MLP\n",
      "\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 282s 5ms/step - loss: 0.7742 - acc: 0.9178 - val_loss: 0.7470 - val_acc: 0.9400\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 269s 4ms/step - loss: 0.6824 - acc: 0.9454 - val_loss: 0.5939 - val_acc: 0.9545\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 274s 5ms/step - loss: 0.5790 - acc: 0.9540 - val_loss: 0.5413 - val_acc: 0.9585\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 276s 5ms/step - loss: 0.5345 - acc: 0.9591 - val_loss: 0.4262 - val_acc: 0.9669\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 289s 5ms/step - loss: 0.4898 - acc: 0.9624 - val_loss: 0.4375 - val_acc: 0.9666\n",
      "Done inference\n",
      "\n",
      " Batch size: 1 \tTrain time: 0.00463285360733668 \tInference time 0.0005607285221417745 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.5780 - acc: 0.9224 - val_loss: 0.5016 - val_acc: 0.9509\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 188s 3ms/step - loss: 0.5566 - acc: 0.9500 - val_loss: 0.4475 - val_acc: 0.9631\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 201s 3ms/step - loss: 0.5362 - acc: 0.9570 - val_loss: 0.4830 - val_acc: 0.9615\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 173s 3ms/step - loss: 0.5034 - acc: 0.9596 - val_loss: 0.4701 - val_acc: 0.9636\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 175s 3ms/step - loss: 0.4833 - acc: 0.9634 - val_loss: 0.4769 - val_acc: 0.9636\n",
      "Done inference\n",
      "\n",
      " Batch size: 2 \tTrain time: 0.0030870140480995178 \tInference time 0.00039501887957255045 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.4849 - acc: 0.9254 - val_loss: 0.4609 - val_acc: 0.9515\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.4855 - acc: 0.9504 - val_loss: 0.4255 - val_acc: 0.9559\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 0.4666 - acc: 0.9581 - val_loss: 0.3554 - val_acc: 0.9690\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.4682 - acc: 0.9603 - val_loss: 0.3657 - val_acc: 0.9687\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.4352 - acc: 0.9635 - val_loss: 0.3345 - val_acc: 0.9727\n",
      "Done inference\n",
      "\n",
      " Batch size: 3 \tTrain time: 0.0019886515307426453 \tInference time 0.00030668368736902875 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.4077 - acc: 0.9258 - val_loss: 0.4552 - val_acc: 0.9479\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.4423 - acc: 0.9510 - val_loss: 0.3903 - val_acc: 0.9604\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.4290 - acc: 0.9583 - val_loss: 0.4088 - val_acc: 0.9628\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.4496 - acc: 0.9621 - val_loss: 0.4833 - val_acc: 0.9574\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.4178 - acc: 0.9652 - val_loss: 0.2939 - val_acc: 0.9738\n",
      "Done inference\n",
      "\n",
      " Batch size: 4 \tTrain time: 0.0015024498089154562 \tInference time 0.00019445170958836872 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 78s 1ms/step - loss: 0.3737 - acc: 0.9284 - val_loss: 0.2453 - val_acc: 0.9576\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.3908 - acc: 0.9511 - val_loss: 0.3431 - val_acc: 0.9622\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.3941 - acc: 0.9580 - val_loss: 0.3403 - val_acc: 0.9660\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 0.3864 - acc: 0.9618 - val_loss: 0.3812 - val_acc: 0.9667\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 0.4094 - acc: 0.9635 - val_loss: 0.3869 - val_acc: 0.9689\n",
      "Done inference\n",
      "\n",
      " Batch size: 5 \tTrain time: 0.0011833875234921773 \tInference time 0.00016256376504898072 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 60s 1ms/step - loss: 0.3482 - acc: 0.9289 - val_loss: 0.2935 - val_acc: 0.9610\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.3606 - acc: 0.9524 - val_loss: 0.2793 - val_acc: 0.9643\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 0.3665 - acc: 0.9593 - val_loss: 0.2799 - val_acc: 0.9687\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.3815 - acc: 0.9611 - val_loss: 0.3160 - val_acc: 0.9686\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.3572 - acc: 0.9655 - val_loss: 0.3714 - val_acc: 0.9651\n",
      "Done inference\n",
      "\n",
      " Batch size: 6 \tTrain time: 0.0011670885038375855 \tInference time 0.00016871166229248048 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 0.3258 - acc: 0.9295 - val_loss: 0.1866 - val_acc: 0.9627\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.3239 - acc: 0.9552 - val_loss: 0.2114 - val_acc: 0.9665\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 0.3421 - acc: 0.9587 - val_loss: 0.2907 - val_acc: 0.9657\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 65s 1ms/step - loss: 0.3326 - acc: 0.9629 - val_loss: 0.3245 - val_acc: 0.9674\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 0.3472 - acc: 0.9645 - val_loss: 0.2866 - val_acc: 0.9737\n",
      "Done inference\n",
      "\n",
      " Batch size: 7 \tTrain time: 0.0011120547199249267 \tInference time 0.00014571091334025064 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 56s 928us/step - loss: 0.3109 - acc: 0.9301 - val_loss: 0.2042 - val_acc: 0.9627\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 51s 856us/step - loss: 0.3025 - acc: 0.9556 - val_loss: 0.2027 - val_acc: 0.9692\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 56s 941us/step - loss: 0.3171 - acc: 0.9607 - val_loss: 0.3392 - val_acc: 0.9620\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 57s 943us/step - loss: 0.3056 - acc: 0.9645 - val_loss: 0.2798 - val_acc: 0.9706\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.3147 - acc: 0.9678 - val_loss: 0.2940 - val_acc: 0.9715\n",
      "Done inference\n",
      "\n",
      " Batch size: 8 \tTrain time: 0.0009452949754397075 \tInference time 0.00012852662007013956 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 54s 907us/step - loss: 0.2842 - acc: 0.9326 - val_loss: 0.1858 - val_acc: 0.9661\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 49s 818us/step - loss: 0.2754 - acc: 0.9564 - val_loss: 0.2787 - val_acc: 0.9665\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 48s 801us/step - loss: 0.2883 - acc: 0.9620 - val_loss: 0.2025 - val_acc: 0.9720\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 45s 747us/step - loss: 0.2962 - acc: 0.9646 - val_loss: 0.2971 - val_acc: 0.9691\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 48s 795us/step - loss: 0.3102 - acc: 0.9677 - val_loss: 0.2778 - val_acc: 0.9707\n",
      "Done inference\n",
      "\n",
      " Batch size: 9 \tTrain time: 0.0008144742250442505 \tInference time 0.00012006571491559346 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 48s 799us/step - loss: 0.2845 - acc: 0.9314 - val_loss: 0.1786 - val_acc: 0.9648\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 50s 832us/step - loss: 0.2616 - acc: 0.9567 - val_loss: 0.2792 - val_acc: 0.9579\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 51s 850us/step - loss: 0.2647 - acc: 0.9627 - val_loss: 0.2650 - val_acc: 0.9669\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 47s 777us/step - loss: 0.2730 - acc: 0.9657 - val_loss: 0.2485 - val_acc: 0.9670\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 45s 746us/step - loss: 0.2805 - acc: 0.9669 - val_loss: 0.2302 - val_acc: 0.9711\n",
      "Done inference\n",
      "\n",
      " Batch size: 10 \tTrain time: 0.0008023828832308452 \tInference time 0.0001123758614063263 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 24s 401us/step - loss: 0.2331 - acc: 0.9345 - val_loss: 0.1466 - val_acc: 0.9662\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 27s 447us/step - loss: 0.1647 - acc: 0.9647 - val_loss: 0.1488 - val_acc: 0.9711\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 29s 481us/step - loss: 0.1692 - acc: 0.9673 - val_loss: 0.1593 - val_acc: 0.9689\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 28s 464us/step - loss: 0.1702 - acc: 0.9712 - val_loss: 0.1540 - val_acc: 0.9734\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 28s 467us/step - loss: 0.1703 - acc: 0.9724 - val_loss: 0.1669 - val_acc: 0.9760\n",
      "Done inference\n",
      "\n",
      " Batch size: 20 \tTrain time: 0.0004529981780052185 \tInference time 7.745010852813721e-05 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 23s 383us/step - loss: 0.2281 - acc: 0.9338 - val_loss: 0.1343 - val_acc: 0.9635\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 22s 374us/step - loss: 0.1382 - acc: 0.9654 - val_loss: 0.1092 - val_acc: 0.9724\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 25s 417us/step - loss: 0.1261 - acc: 0.9714 - val_loss: 0.1492 - val_acc: 0.9733\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.1231 - acc: 0.9741 - val_loss: 0.1448 - val_acc: 0.9738\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 0.1180 - acc: 0.9773 - val_loss: 0.1392 - val_acc: 0.9752\n",
      "Done inference\n",
      "\n",
      " Batch size: 30 \tTrain time: 0.00038472362995147706 \tInference time 6.926493909623888e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.2224 - acc: 0.9339 - val_loss: 0.1104 - val_acc: 0.9675\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.1223 - acc: 0.9681 - val_loss: 0.1195 - val_acc: 0.9700\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.1088 - acc: 0.9730 - val_loss: 0.1400 - val_acc: 0.9705\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.1031 - acc: 0.9764 - val_loss: 0.1127 - val_acc: 0.9758\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.0961 - acc: 0.9788 - val_loss: 0.1248 - val_acc: 0.9760\n",
      "Done inference\n",
      "\n",
      " Batch size: 40 \tTrain time: 0.00029175181786219277 \tInference time 5.931078460481432e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.2218 - acc: 0.9330 - val_loss: 0.1108 - val_acc: 0.9652\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: 0.1108 - acc: 0.9693 - val_loss: 0.1035 - val_acc: 0.9723\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 0.0937 - acc: 0.9757 - val_loss: 0.1080 - val_acc: 0.9768\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.0881 - acc: 0.9792 - val_loss: 0.1009 - val_acc: 0.9794\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0802 - acc: 0.9810 - val_loss: 0.1088 - val_acc: 0.9785\n",
      "Done inference\n",
      "\n",
      " Batch size: 50 \tTrain time: 0.00029828756809234617 \tInference time 6.317333380381266e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.2207 - acc: 0.9326 - val_loss: 0.1188 - val_acc: 0.9652\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.1082 - acc: 0.9691 - val_loss: 0.1065 - val_acc: 0.9714\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.0897 - acc: 0.9754 - val_loss: 0.1024 - val_acc: 0.9749\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0750 - acc: 0.9800 - val_loss: 0.1197 - val_acc: 0.9694\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0700 - acc: 0.9825 - val_loss: 0.0989 - val_acc: 0.9794\n",
      "Done inference\n",
      "\n",
      " Batch size: 60 \tTrain time: 0.0002544802149136861 \tInference time 5.5756955676608616e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.2254 - acc: 0.9306 - val_loss: 0.1042 - val_acc: 0.9676\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.1064 - acc: 0.9689 - val_loss: 0.0961 - val_acc: 0.9741\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0808 - acc: 0.9763 - val_loss: 0.0852 - val_acc: 0.9764\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0682 - acc: 0.9809 - val_loss: 0.0877 - val_acc: 0.9785\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0610 - acc: 0.9835 - val_loss: 0.0865 - val_acc: 0.9799\n",
      "Done inference\n",
      "\n",
      " Batch size: 70 \tTrain time: 0.00024396814902623493 \tInference time 5.8638136916690405e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.2317 - acc: 0.9302 - val_loss: 0.0936 - val_acc: 0.9719\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.1042 - acc: 0.9688 - val_loss: 0.0783 - val_acc: 0.9764\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0798 - acc: 0.9767 - val_loss: 0.0910 - val_acc: 0.9744\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0668 - acc: 0.9816 - val_loss: 0.0754 - val_acc: 0.9796\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0595 - acc: 0.9841 - val_loss: 0.0837 - val_acc: 0.9805\n",
      "Done inference\n",
      "\n",
      " Batch size: 80 \tTrain time: 0.00022888947010040283 \tInference time 5.417464044358995e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.2316 - acc: 0.9286 - val_loss: 0.1097 - val_acc: 0.9638\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.1010 - acc: 0.9692 - val_loss: 0.0807 - val_acc: 0.9760\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.0785 - acc: 0.9771 - val_loss: 0.1058 - val_acc: 0.9728\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.0667 - acc: 0.9813 - val_loss: 0.0908 - val_acc: 0.9781\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0580 - acc: 0.9843 - val_loss: 0.0828 - val_acc: 0.9817\n",
      "Done inference\n",
      "\n",
      " Batch size: 90 \tTrain time: 0.0002142064627011617 \tInference time 5.1415328184763594e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 0.2318 - acc: 0.9295 - val_loss: 0.1052 - val_acc: 0.9670\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.1009 - acc: 0.9694 - val_loss: 0.0896 - val_acc: 0.9732\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0748 - acc: 0.9777 - val_loss: 0.0788 - val_acc: 0.9795\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0620 - acc: 0.9817 - val_loss: 0.0786 - val_acc: 0.9784\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0540 - acc: 0.9843 - val_loss: 0.0743 - val_acc: 0.9815\n",
      "Done inference\n",
      "\n",
      " Batch size: 100 \tTrain time: 0.000197178529103597 \tInference time 5.233177741368612e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.2711 - acc: 0.9170 - val_loss: 0.1274 - val_acc: 0.9586\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.1070 - acc: 0.9667 - val_loss: 0.0792 - val_acc: 0.9770\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0769 - acc: 0.9768 - val_loss: 0.0736 - val_acc: 0.9782\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0574 - acc: 0.9815 - val_loss: 0.0674 - val_acc: 0.9796\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0482 - acc: 0.9845 - val_loss: 0.0712 - val_acc: 0.9808\n",
      "Done inference\n",
      "\n",
      " Batch size: 200 \tTrain time: 0.0001714441657066345 \tInference time 4.712630377875434e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.3132 - acc: 0.9037 - val_loss: 0.1409 - val_acc: 0.9550\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.1178 - acc: 0.9626 - val_loss: 0.0861 - val_acc: 0.9737\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0796 - acc: 0.9754 - val_loss: 0.0704 - val_acc: 0.9783\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0616 - acc: 0.9807 - val_loss: 0.0748 - val_acc: 0.9783\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0498 - acc: 0.9847 - val_loss: 0.0719 - val_acc: 0.9786\n",
      "Done inference\n",
      "\n",
      " Batch size: 300 \tTrain time: 0.0001579070727030436 \tInference time 4.622253179550171e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.3424 - acc: 0.8944 - val_loss: 0.1354 - val_acc: 0.9586\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1265 - acc: 0.9619 - val_loss: 0.0969 - val_acc: 0.9699\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0846 - acc: 0.9734 - val_loss: 0.0752 - val_acc: 0.9764\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0628 - acc: 0.9804 - val_loss: 0.0664 - val_acc: 0.9809\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0498 - acc: 0.9845 - val_loss: 0.0676 - val_acc: 0.9806\n",
      "Done inference\n",
      "\n",
      " Batch size: 400 \tTrain time: 0.00015446025053660075 \tInference time 4.541888369454278e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.3738 - acc: 0.8848 - val_loss: 0.1444 - val_acc: 0.9531\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1377 - acc: 0.9581 - val_loss: 0.1143 - val_acc: 0.9620\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0921 - acc: 0.9714 - val_loss: 0.0842 - val_acc: 0.9740\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0689 - acc: 0.9783 - val_loss: 0.0733 - val_acc: 0.9771\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0528 - acc: 0.9834 - val_loss: 0.0672 - val_acc: 0.9792\n",
      "Done inference\n",
      "\n",
      " Batch size: 500 \tTrain time: 0.00014787416617075602 \tInference time 4.4637062814500596e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.4089 - acc: 0.8737 - val_loss: 0.1854 - val_acc: 0.9414\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.1538 - acc: 0.9537 - val_loss: 0.0988 - val_acc: 0.9689\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1016 - acc: 0.9688 - val_loss: 0.0879 - val_acc: 0.9724\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0744 - acc: 0.9771 - val_loss: 0.0689 - val_acc: 0.9789\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0572 - acc: 0.9822 - val_loss: 0.0663 - val_acc: 0.9806\n",
      "Done inference\n",
      "\n",
      " Batch size: 600 \tTrain time: 0.00014832123120625814 \tInference time 4.675558143191867e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.4360 - acc: 0.8662 - val_loss: 0.2003 - val_acc: 0.9384\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1627 - acc: 0.9506 - val_loss: 0.1061 - val_acc: 0.9680\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1076 - acc: 0.9668 - val_loss: 0.0862 - val_acc: 0.9733\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0794 - acc: 0.9752 - val_loss: 0.0715 - val_acc: 0.9779\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0613 - acc: 0.9811 - val_loss: 0.0646 - val_acc: 0.9796\n",
      "Done inference\n",
      "\n",
      " Batch size: 700 \tTrain time: 0.00014903778076171876 \tInference time 4.4897629155053035e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.4602 - acc: 0.8575 - val_loss: 0.1934 - val_acc: 0.9402\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.1734 - acc: 0.9470 - val_loss: 0.1203 - val_acc: 0.9615\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.1174 - acc: 0.9646 - val_loss: 0.0910 - val_acc: 0.9713\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0856 - acc: 0.9728 - val_loss: 0.0878 - val_acc: 0.9736\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0674 - acc: 0.9785 - val_loss: 0.0738 - val_acc: 0.9768\n",
      "Done inference\n",
      "\n",
      " Batch size: 800 \tTrain time: 0.00014415589888890583 \tInference time 4.344169994195302e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.5026 - acc: 0.8428 - val_loss: 0.1983 - val_acc: 0.9398\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.1922 - acc: 0.9421 - val_loss: 0.1347 - val_acc: 0.9575\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.1319 - acc: 0.9595 - val_loss: 0.1015 - val_acc: 0.9699\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.0937 - acc: 0.9715 - val_loss: 0.0783 - val_acc: 0.9761\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0723 - acc: 0.9773 - val_loss: 0.0847 - val_acc: 0.9748\n",
      "Done inference\n",
      "\n",
      " Batch size: 900 \tTrain time: 0.00013813772281010946 \tInference time 4.3009133140246076e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.5143 - acc: 0.8385 - val_loss: 0.2104 - val_acc: 0.9359\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1991 - acc: 0.9400 - val_loss: 0.1282 - val_acc: 0.9603\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.1314 - acc: 0.9598 - val_loss: 0.1058 - val_acc: 0.9672\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0958 - acc: 0.9706 - val_loss: 0.0922 - val_acc: 0.9710\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0749 - acc: 0.9766 - val_loss: 0.0708 - val_acc: 0.9767\n",
      "Done inference\n",
      "\n",
      " Batch size: 1000 \tTrain time: 0.00013527613162994385 \tInference time 4.3336531519889835e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.7143 - acc: 0.7721 - val_loss: 0.3523 - val_acc: 0.8904\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.2963 - acc: 0.9099 - val_loss: 0.2117 - val_acc: 0.9353\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.2115 - acc: 0.9354 - val_loss: 0.2406 - val_acc: 0.9221\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.1676 - acc: 0.9482 - val_loss: 0.1136 - val_acc: 0.9646\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.1360 - acc: 0.9588 - val_loss: 0.1046 - val_acc: 0.9673\n",
      "Done inference\n",
      "\n",
      " Batch size: 2000 \tTrain time: 0.0001311449098587036 \tInference time 4.224711457888285e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.8555 - acc: 0.7361 - val_loss: 0.3158 - val_acc: 0.9133\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.3469 - acc: 0.8945 - val_loss: 0.2423 - val_acc: 0.9260\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2612 - acc: 0.9195 - val_loss: 0.2210 - val_acc: 0.9316\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2125 - acc: 0.9355 - val_loss: 0.1869 - val_acc: 0.9400\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.1834 - acc: 0.9441 - val_loss: 0.1318 - val_acc: 0.9582\n",
      "Done inference\n",
      "\n",
      " Batch size: 3000 \tTrain time: 0.00012872459252675375 \tInference time 4.2954294880231225e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 1.0096 - acc: 0.6754 - val_loss: 0.4249 - val_acc: 0.8641\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.4189 - acc: 0.8702 - val_loss: 0.2728 - val_acc: 0.9221\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3082 - acc: 0.9046 - val_loss: 0.2228 - val_acc: 0.9341\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.2611 - acc: 0.9201 - val_loss: 0.1884 - val_acc: 0.9444\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.2018 - acc: 0.9398 - val_loss: 0.1652 - val_acc: 0.9493\n",
      "Done inference\n",
      "\n",
      " Batch size: 4000 \tTrain time: 0.00012759259462356569 \tInference time 4.31203564008077e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 1.0855 - acc: 0.6550 - val_loss: 0.4808 - val_acc: 0.8548\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.4399 - acc: 0.8668 - val_loss: 0.3364 - val_acc: 0.8970\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.3361 - acc: 0.8969 - val_loss: 0.2896 - val_acc: 0.9107\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.2700 - acc: 0.9189 - val_loss: 0.2636 - val_acc: 0.9163\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.2473 - acc: 0.9251 - val_loss: 0.1976 - val_acc: 0.9377\n",
      "Done inference\n",
      "\n",
      " Batch size: 5000 \tTrain time: 0.00013218597412109376 \tInference time 4.0772426128387454e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 1.1870 - acc: 0.6168 - val_loss: 0.6084 - val_acc: 0.7915\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.5043 - acc: 0.8432 - val_loss: 0.3303 - val_acc: 0.9053\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.3846 - acc: 0.8821 - val_loss: 0.3412 - val_acc: 0.8972\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3151 - acc: 0.9051 - val_loss: 0.2219 - val_acc: 0.9341\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.2785 - acc: 0.9169 - val_loss: 0.2018 - val_acc: 0.9381\n",
      "Done inference\n",
      "\n",
      " Batch size: 6000 \tTrain time: 0.00013454809506734211 \tInference time 4.1637399792671206e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 1.2733 - acc: 0.5957 - val_loss: 0.6352 - val_acc: 0.7973\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.5451 - acc: 0.8335 - val_loss: 0.4743 - val_acc: 0.8368\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.3797 - acc: 0.8857 - val_loss: 0.3692 - val_acc: 0.8828\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.3475 - acc: 0.8942 - val_loss: 0.2421 - val_acc: 0.9278\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.3137 - acc: 0.9013 - val_loss: 0.2573 - val_acc: 0.9206\n",
      "Done inference\n",
      "\n",
      " Batch size: 7000 \tTrain time: 0.0001388829509417216 \tInference time 4.585602945751614e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 1.3319 - acc: 0.5706 - val_loss: 0.5605 - val_acc: 0.8516\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.6225 - acc: 0.8077 - val_loss: 0.3710 - val_acc: 0.8929\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.4017 - acc: 0.8797 - val_loss: 0.3124 - val_acc: 0.9086\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.3534 - acc: 0.8902 - val_loss: 0.3229 - val_acc: 0.9031\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3045 - acc: 0.9090 - val_loss: 0.2531 - val_acc: 0.9233\n",
      "Done inference\n",
      "\n",
      " Batch size: 8000 \tTrain time: 0.0001395345679918925 \tInference time 4.52654759089152e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 1.4310 - acc: 0.5292 - val_loss: 0.7192 - val_acc: 0.7554\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.6370 - acc: 0.7986 - val_loss: 0.4153 - val_acc: 0.8775\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.4417 - acc: 0.8651 - val_loss: 0.3279 - val_acc: 0.9025\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.3905 - acc: 0.8783 - val_loss: 0.2853 - val_acc: 0.9149\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.3333 - acc: 0.8987 - val_loss: 0.2656 - val_acc: 0.9230\n",
      "Done inference\n",
      "\n",
      " Batch size: 9000 \tTrain time: 0.00013844428618748982 \tInference time 4.459146062533061e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 1.4856 - acc: 0.5026 - val_loss: 0.8116 - val_acc: 0.7237\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.7005 - acc: 0.7847 - val_loss: 0.5053 - val_acc: 0.8349\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.4748 - acc: 0.8535 - val_loss: 0.4549 - val_acc: 0.8527\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.4118 - acc: 0.8721 - val_loss: 0.2990 - val_acc: 0.9144\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.3878 - acc: 0.8768 - val_loss: 0.2616 - val_acc: 0.9290\n",
      "Done inference\n",
      "\n",
      " Batch size: 10000 \tTrain time: 0.00013860533952713013 \tInference time 4.4684839248657224e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "TRAIN ON GPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.7819 - acc: 0.9175 - val_loss: 0.6079 - val_acc: 0.9441\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.6456 - acc: 0.9464 - val_loss: 0.3999 - val_acc: 0.9653\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.5608 - acc: 0.9554 - val_loss: 0.4589 - val_acc: 0.9639\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 244s 4ms/step - loss: 0.5203 - acc: 0.9589 - val_loss: 0.4350 - val_acc: 0.9659\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 247s 4ms/step - loss: 0.4815 - acc: 0.9627 - val_loss: 0.4455 - val_acc: 0.9670\n",
      "Done inference\n",
      "\n",
      " Batch size: 1 \tTrain time: 0.00404872907559077 \tInference time 0.0006078497568766276 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.5684 - acc: 0.9226 - val_loss: 0.5869 - val_acc: 0.9439\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.5580 - acc: 0.9487 - val_loss: 0.3719 - val_acc: 0.9651\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.5339 - acc: 0.9554 - val_loss: 0.4063 - val_acc: 0.9676\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.4919 - acc: 0.9608 - val_loss: 0.4616 - val_acc: 0.9650\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 127s 2ms/step - loss: 0.4644 - acc: 0.9636 - val_loss: 0.3898 - val_acc: 0.9706\n",
      "Done inference\n",
      "\n",
      " Batch size: 2 \tTrain time: 0.0021736720434824626 \tInference time 0.00029731813669204714 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.4818 - acc: 0.9235 - val_loss: 0.4250 - val_acc: 0.9509\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.4963 - acc: 0.9503 - val_loss: 0.4188 - val_acc: 0.9628\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.4731 - acc: 0.9569 - val_loss: 0.3954 - val_acc: 0.9673\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.4705 - acc: 0.9610 - val_loss: 0.3954 - val_acc: 0.9670\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.4379 - acc: 0.9643 - val_loss: 0.3835 - val_acc: 0.9701\n",
      "Done inference\n",
      "\n",
      " Batch size: 3 \tTrain time: 0.0013738627592722575 \tInference time 0.00020821809371312459 (1)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.4223 - acc: 0.9253 - val_loss: 0.3628 - val_acc: 0.9527\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.4260 - acc: 0.9518 - val_loss: 0.3820 - val_acc: 0.9549\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.4531 - acc: 0.9571 - val_loss: 0.3382 - val_acc: 0.9714\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 65s 1ms/step - loss: 0.4307 - acc: 0.9624 - val_loss: 0.3233 - val_acc: 0.9707\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.3868 - acc: 0.9667 - val_loss: 0.3373 - val_acc: 0.9718\n",
      "Done inference\n",
      "\n",
      " Batch size: 4 \tTrain time: 0.0010584680398305257 \tInference time 0.00015274801850318907 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 51s 846us/step - loss: 0.3710 - acc: 0.9276 - val_loss: 0.3158 - val_acc: 0.9555\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 51s 843us/step - loss: 0.4034 - acc: 0.9528 - val_loss: 0.2945 - val_acc: 0.9677\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 52s 866us/step - loss: 0.3877 - acc: 0.9590 - val_loss: 0.3319 - val_acc: 0.9654\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 50s 840us/step - loss: 0.3955 - acc: 0.9630 - val_loss: 0.4109 - val_acc: 0.9647\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 51s 844us/step - loss: 0.3917 - acc: 0.9646 - val_loss: 0.2974 - val_acc: 0.9708\n",
      "Done inference\n",
      "\n",
      " Batch size: 5 \tTrain time: 0.0008494894766807556 \tInference time 0.00012337908744812013 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 41s 687us/step - loss: 0.3519 - acc: 0.9273 - val_loss: 0.2103 - val_acc: 0.9629\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 42s 697us/step - loss: 0.3577 - acc: 0.9528 - val_loss: 0.2547 - val_acc: 0.9648\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 42s 695us/step - loss: 0.3855 - acc: 0.9573 - val_loss: 0.2728 - val_acc: 0.9675\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 44s 741us/step - loss: 0.3728 - acc: 0.9615 - val_loss: 0.3603 - val_acc: 0.9637\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 43s 712us/step - loss: 0.3809 - acc: 0.9641 - val_loss: 0.4216 - val_acc: 0.9642\n",
      "Done inference\n",
      "\n",
      " Batch size: 6 \tTrain time: 0.0007073731883366903 \tInference time 0.00010258510112762451 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 37s 611us/step - loss: 0.3240 - acc: 0.9308 - val_loss: 0.2793 - val_acc: 0.9542\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 36s 602us/step - loss: 0.3267 - acc: 0.9529 - val_loss: 0.2694 - val_acc: 0.9624\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 38s 626us/step - loss: 0.3355 - acc: 0.9596 - val_loss: 0.2588 - val_acc: 0.9663\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 36s 598us/step - loss: 0.3437 - acc: 0.9632 - val_loss: 0.3186 - val_acc: 0.9672\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 37s 612us/step - loss: 0.3509 - acc: 0.9656 - val_loss: 0.3094 - val_acc: 0.9689\n",
      "Done inference\n",
      "\n",
      " Batch size: 7 \tTrain time: 0.000611465265750885 \tInference time 9.054846366246541e-05 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 33s 548us/step - loss: 0.3022 - acc: 0.9324 - val_loss: 0.2265 - val_acc: 0.9610\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 32s 536us/step - loss: 0.3005 - acc: 0.9540 - val_loss: 0.2412 - val_acc: 0.9648\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 31s 521us/step - loss: 0.3137 - acc: 0.9589 - val_loss: 0.3693 - val_acc: 0.9557\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 30s 507us/step - loss: 0.3196 - acc: 0.9641 - val_loss: 0.3337 - val_acc: 0.9673\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 31s 520us/step - loss: 0.3298 - acc: 0.9660 - val_loss: 0.2583 - val_acc: 0.9738\n",
      "Done inference\n",
      "\n",
      " Batch size: 8 \tTrain time: 0.0005274283957481384 \tInference time 7.262385686238607e-05 (2)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 25s 423us/step - loss: 0.3017 - acc: 0.9303 - val_loss: 0.2233 - val_acc: 0.9580\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 25s 423us/step - loss: 0.2825 - acc: 0.9567 - val_loss: 0.1990 - val_acc: 0.9676\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 25s 412us/step - loss: 0.2968 - acc: 0.9603 - val_loss: 0.2509 - val_acc: 0.9708\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 26s 431us/step - loss: 0.3101 - acc: 0.9639 - val_loss: 0.3651 - val_acc: 0.9592\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 25s 418us/step - loss: 0.3125 - acc: 0.9661 - val_loss: 0.2603 - val_acc: 0.9729\n",
      "Done inference\n",
      "\n",
      " Batch size: 9 \tTrain time: 0.0004222261953353882 \tInference time 6.633789274427626e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 23s 390us/step - loss: 0.2822 - acc: 0.9331 - val_loss: 0.2528 - val_acc: 0.9573\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 24s 398us/step - loss: 0.2690 - acc: 0.9574 - val_loss: 0.1765 - val_acc: 0.9720\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 23s 391us/step - loss: 0.2734 - acc: 0.9613 - val_loss: 0.2900 - val_acc: 0.9657\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 23s 384us/step - loss: 0.2859 - acc: 0.9649 - val_loss: 0.3123 - val_acc: 0.9673\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 23s 382us/step - loss: 0.2827 - acc: 0.9675 - val_loss: 0.2436 - val_acc: 0.9711\n",
      "Done inference\n",
      "\n",
      " Batch size: 10 \tTrain time: 0.0003895996753374736 \tInference time 5.8977276749081084e-05 (3)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.2335 - acc: 0.9352 - val_loss: 0.1398 - val_acc: 0.9652\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.1645 - acc: 0.9644 - val_loss: 0.1494 - val_acc: 0.9707\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.1609 - acc: 0.9687 - val_loss: 0.1535 - val_acc: 0.9737\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.1646 - acc: 0.9713 - val_loss: 0.1741 - val_acc: 0.9712\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.1741 - acc: 0.9725 - val_loss: 0.1722 - val_acc: 0.9753\n",
      "Done inference\n",
      "\n",
      " Batch size: 20 \tTrain time: 0.00019369928201039632 \tInference time 3.504659632841746e-05 (4)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.2230 - acc: 0.9354 - val_loss: 0.1229 - val_acc: 0.9676\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.1349 - acc: 0.9672 - val_loss: 0.1214 - val_acc: 0.9702\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.1241 - acc: 0.9716 - val_loss: 0.1380 - val_acc: 0.9739\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.1210 - acc: 0.9753 - val_loss: 0.1413 - val_acc: 0.9750\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.1256 - acc: 0.9765 - val_loss: 0.1286 - val_acc: 0.9754\n",
      "Done inference\n",
      "\n",
      " Batch size: 30 \tTrain time: 0.0001330933650334676 \tInference time 2.5301330884297687e-05 (5)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2202 - acc: 0.9343 - val_loss: 0.1000 - val_acc: 0.9707\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.1182 - acc: 0.9683 - val_loss: 0.1084 - val_acc: 0.9735\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.1065 - acc: 0.9735 - val_loss: 0.1140 - val_acc: 0.9724\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1008 - acc: 0.9776 - val_loss: 0.1338 - val_acc: 0.9748\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0989 - acc: 0.9784 - val_loss: 0.1072 - val_acc: 0.9801\n",
      "Done inference\n",
      "\n",
      " Batch size: 40 \tTrain time: 0.00010101508696873982 \tInference time 1.8209154076046414e-05 (6)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2217 - acc: 0.9334 - val_loss: 0.1190 - val_acc: 0.9655\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1117 - acc: 0.9684 - val_loss: 0.1116 - val_acc: 0.9693\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0976 - acc: 0.9758 - val_loss: 0.0920 - val_acc: 0.9778\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0841 - acc: 0.9790 - val_loss: 0.0956 - val_acc: 0.9798\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0844 - acc: 0.9811 - val_loss: 0.1028 - val_acc: 0.9801\n",
      "Done inference\n",
      "\n",
      " Batch size: 50 \tTrain time: 8.024230639139811e-05 \tInference time 1.5557470208122618e-05 (7)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2211 - acc: 0.9323 - val_loss: 0.1114 - val_acc: 0.9677\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1083 - acc: 0.9692 - val_loss: 0.0903 - val_acc: 0.9750\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0860 - acc: 0.9760 - val_loss: 0.0898 - val_acc: 0.9778\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0786 - acc: 0.9794 - val_loss: 0.0861 - val_acc: 0.9787\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0717 - acc: 0.9818 - val_loss: 0.0995 - val_acc: 0.9790\n",
      "Done inference\n",
      "\n",
      " Batch size: 60 \tTrain time: 6.55035392443339e-05 \tInference time 1.2837229172388712e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2260 - acc: 0.9318 - val_loss: 0.1346 - val_acc: 0.9617\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 58us/step - loss: 0.1068 - acc: 0.9681 - val_loss: 0.1007 - val_acc: 0.9687\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0832 - acc: 0.9772 - val_loss: 0.0914 - val_acc: 0.9759\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0723 - acc: 0.9807 - val_loss: 0.0846 - val_acc: 0.9783\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0612 - acc: 0.9833 - val_loss: 0.1051 - val_acc: 0.9779\n",
      "Done inference\n",
      "\n",
      " Batch size: 70 \tTrain time: 5.7511791388193766e-05 \tInference time 1.172607938448588e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.2260 - acc: 0.9314 - val_loss: 0.1030 - val_acc: 0.9681\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.1018 - acc: 0.9696 - val_loss: 0.0903 - val_acc: 0.9729\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0802 - acc: 0.9769 - val_loss: 0.0842 - val_acc: 0.9764\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0664 - acc: 0.9808 - val_loss: 0.0837 - val_acc: 0.9802\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0596 - acc: 0.9839 - val_loss: 0.0744 - val_acc: 0.9839\n",
      "Done inference\n",
      "\n",
      " Batch size: 80 \tTrain time: 5.007208585739136e-05 \tInference time 1.1071547865867615e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2351 - acc: 0.9279 - val_loss: 0.1015 - val_acc: 0.9688\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1039 - acc: 0.9690 - val_loss: 0.0885 - val_acc: 0.9733\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0809 - acc: 0.9768 - val_loss: 0.0803 - val_acc: 0.9774\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0656 - acc: 0.9815 - val_loss: 0.0784 - val_acc: 0.9791\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0546 - acc: 0.9841 - val_loss: 0.0858 - val_acc: 0.9797\n",
      "Done inference\n",
      "\n",
      " Batch size: 90 \tTrain time: 4.683149814605713e-05 \tInference time 1.0424550374348959e-05 (8)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2358 - acc: 0.9281 - val_loss: 0.1163 - val_acc: 0.9635\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1025 - acc: 0.9690 - val_loss: 0.0720 - val_acc: 0.9779\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0761 - acc: 0.9776 - val_loss: 0.0838 - val_acc: 0.9770\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0637 - acc: 0.9812 - val_loss: 0.0807 - val_acc: 0.9786\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0566 - acc: 0.9844 - val_loss: 0.0704 - val_acc: 0.9827\n",
      "Done inference\n",
      "\n",
      " Batch size: 100 \tTrain time: 3.942339022954305e-05 \tInference time 9.120073583390978e-06 (9)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.2748 - acc: 0.9158 - val_loss: 0.1169 - val_acc: 0.9622\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1074 - acc: 0.9676 - val_loss: 0.1077 - val_acc: 0.9671\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0762 - acc: 0.9767 - val_loss: 0.0778 - val_acc: 0.9766\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0589 - acc: 0.9819 - val_loss: 0.0730 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0466 - acc: 0.9853 - val_loss: 0.0718 - val_acc: 0.9803\n",
      "Done inference\n",
      "\n",
      " Batch size: 200 \tTrain time: 2.2523438930511476e-05 \tInference time 5.927848093437426e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3105 - acc: 0.9029 - val_loss: 0.1378 - val_acc: 0.9563\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1150 - acc: 0.9646 - val_loss: 0.0841 - val_acc: 0.9732\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0771 - acc: 0.9761 - val_loss: 0.0761 - val_acc: 0.9772\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0601 - acc: 0.9808 - val_loss: 0.0685 - val_acc: 0.9783\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0467 - acc: 0.9850 - val_loss: 0.0658 - val_acc: 0.9796\n",
      "Done inference\n",
      "\n",
      " Batch size: 300 \tTrain time: 1.637341578801473e-05 \tInference time 4.755939498092189e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3409 - acc: 0.8958 - val_loss: 0.1574 - val_acc: 0.9504\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.1281 - acc: 0.9607 - val_loss: 0.0894 - val_acc: 0.9732\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0845 - acc: 0.9735 - val_loss: 0.0767 - val_acc: 0.9768\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0638 - acc: 0.9801 - val_loss: 0.0783 - val_acc: 0.9770\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.0496 - acc: 0.9841 - val_loss: 0.0717 - val_acc: 0.9788\n",
      "Done inference\n",
      "\n",
      " Batch size: 400 \tTrain time: 1.3467741012573242e-05 \tInference time 4.496771277803363e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3745 - acc: 0.8849 - val_loss: 0.1418 - val_acc: 0.9566\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.1370 - acc: 0.9581 - val_loss: 0.0963 - val_acc: 0.9687\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0923 - acc: 0.9712 - val_loss: 0.0754 - val_acc: 0.9763\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0668 - acc: 0.9790 - val_loss: 0.0738 - val_acc: 0.9762\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.0540 - acc: 0.9830 - val_loss: 0.0706 - val_acc: 0.9790\n",
      "Done inference\n",
      "\n",
      " Batch size: 500 \tTrain time: 1.2243537108103434e-05 \tInference time 4.051435354984168e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.4111 - acc: 0.8732 - val_loss: 0.1499 - val_acc: 0.9533\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1532 - acc: 0.9532 - val_loss: 0.1140 - val_acc: 0.9629\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1021 - acc: 0.9686 - val_loss: 0.0866 - val_acc: 0.9726\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0740 - acc: 0.9770 - val_loss: 0.0881 - val_acc: 0.9734\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0579 - acc: 0.9818 - val_loss: 0.0662 - val_acc: 0.9797\n",
      "Done inference\n",
      "\n",
      " Batch size: 600 \tTrain time: 1.0306185881296794e-05 \tInference time 3.6934071116977266e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.4263 - acc: 0.8670 - val_loss: 0.1616 - val_acc: 0.9517\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1615 - acc: 0.9511 - val_loss: 0.1151 - val_acc: 0.9644\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1062 - acc: 0.9672 - val_loss: 0.0886 - val_acc: 0.9727\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0786 - acc: 0.9756 - val_loss: 0.0966 - val_acc: 0.9684\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0612 - acc: 0.9814 - val_loss: 0.0793 - val_acc: 0.9754\n",
      "Done inference\n",
      "\n",
      " Batch size: 700 \tTrain time: 9.374789396921794e-06 \tInference time 3.716956575711568e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.4581 - acc: 0.8564 - val_loss: 0.1831 - val_acc: 0.9447\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1761 - acc: 0.9453 - val_loss: 0.1199 - val_acc: 0.9606\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1168 - acc: 0.9640 - val_loss: 0.0953 - val_acc: 0.9714\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0871 - acc: 0.9726 - val_loss: 0.0752 - val_acc: 0.9764\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0661 - acc: 0.9785 - val_loss: 0.0704 - val_acc: 0.9783\n",
      "Done inference\n",
      "\n",
      " Batch size: 800 \tTrain time: 8.592690626780192e-06 \tInference time 3.686127879402854e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.4876 - acc: 0.8482 - val_loss: 0.2061 - val_acc: 0.9357\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1870 - acc: 0.9434 - val_loss: 0.1320 - val_acc: 0.9611\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1265 - acc: 0.9617 - val_loss: 0.0927 - val_acc: 0.9719\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0939 - acc: 0.9706 - val_loss: 0.0869 - val_acc: 0.9728\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0695 - acc: 0.9783 - val_loss: 0.0760 - val_acc: 0.9763\n",
      "Done inference\n",
      "\n",
      " Batch size: 900 \tTrain time: 8.687284787495932e-06 \tInference time 3.4788896640141806e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.5116 - acc: 0.8405 - val_loss: 0.2362 - val_acc: 0.9277\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1997 - acc: 0.9392 - val_loss: 0.1496 - val_acc: 0.9534\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1356 - acc: 0.9576 - val_loss: 0.1125 - val_acc: 0.9630\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0981 - acc: 0.9702 - val_loss: 0.0995 - val_acc: 0.9682\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.0772 - acc: 0.9758 - val_loss: 0.0757 - val_acc: 0.9757\n",
      "Done inference\n",
      "\n",
      " Batch size: 1000 \tTrain time: 8.486604690551757e-06 \tInference time 3.5086335557879825e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.7036 - acc: 0.7816 - val_loss: 0.3302 - val_acc: 0.8908\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2876 - acc: 0.9126 - val_loss: 0.2980 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2000 - acc: 0.9398 - val_loss: 0.2076 - val_acc: 0.9315\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.1577 - acc: 0.9516 - val_loss: 0.1233 - val_acc: 0.9602\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.1315 - acc: 0.9596 - val_loss: 0.1260 - val_acc: 0.9586\n",
      "Done inference\n",
      "\n",
      " Batch size: 2000 \tTrain time: 7.317152023315429e-06 \tInference time 3.765739334954156e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.8466 - acc: 0.7300 - val_loss: 0.3930 - val_acc: 0.8830\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3435 - acc: 0.8975 - val_loss: 0.2652 - val_acc: 0.9199\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2703 - acc: 0.9163 - val_loss: 0.1941 - val_acc: 0.9401\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2158 - acc: 0.9340 - val_loss: 0.2238 - val_acc: 0.9297\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.1697 - acc: 0.9492 - val_loss: 0.1450 - val_acc: 0.9548\n",
      "Done inference\n",
      "\n",
      " Batch size: 3000 \tTrain time: 5.612080097198486e-06 \tInference time 4.391465403816917e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.9745 - acc: 0.6910 - val_loss: 0.4635 - val_acc: 0.8470\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.4227 - acc: 0.8718 - val_loss: 0.2615 - val_acc: 0.9240\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3189 - acc: 0.9014 - val_loss: 0.2203 - val_acc: 0.9356\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2468 - acc: 0.9272 - val_loss: 0.2240 - val_acc: 0.9294\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2121 - acc: 0.9345 - val_loss: 0.1465 - val_acc: 0.9550\n",
      "Done inference\n",
      "\n",
      " Batch size: 4000 \tTrain time: 6.669041315714518e-06 \tInference time 3.914385881179419e-06 (13)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.0936 - acc: 0.6486 - val_loss: 0.6084 - val_acc: 0.7882\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4623 - acc: 0.8571 - val_loss: 0.3067 - val_acc: 0.9123\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3167 - acc: 0.9052 - val_loss: 0.2845 - val_acc: 0.9079\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3023 - acc: 0.9049 - val_loss: 0.2157 - val_acc: 0.9336\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2455 - acc: 0.9250 - val_loss: 0.2000 - val_acc: 0.9353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done inference\n",
      "\n",
      " Batch size: 5000 \tTrain time: 5.650022029876709e-06 \tInference time 3.910848727593055e-06 (13)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 1.1729 - acc: 0.6287 - val_loss: 0.6625 - val_acc: 0.7830\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.5264 - acc: 0.8339 - val_loss: 0.3723 - val_acc: 0.8885\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3565 - acc: 0.8940 - val_loss: 0.3078 - val_acc: 0.9038\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3281 - acc: 0.8979 - val_loss: 0.2165 - val_acc: 0.9372\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.2786 - acc: 0.9171 - val_loss: 0.2065 - val_acc: 0.9374\n",
      "Done inference\n",
      "\n",
      " Batch size: 6000 \tTrain time: 6.9256997108459475e-06 \tInference time 4.416551854875353e-06 (12)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 1.2761 - acc: 0.5930 - val_loss: 0.5852 - val_acc: 0.7982\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.5472 - acc: 0.8273 - val_loss: 0.4285 - val_acc: 0.8706\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3944 - acc: 0.8812 - val_loss: 0.3025 - val_acc: 0.9095\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3341 - acc: 0.8966 - val_loss: 0.2483 - val_acc: 0.9261\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.2861 - acc: 0.9155 - val_loss: 0.2469 - val_acc: 0.9246\n",
      "Done inference\n",
      "\n",
      " Batch size: 7000 \tTrain time: 6.716055075327555e-06 \tInference time 7.853390552379466e-06 (9)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.3832 - acc: 0.5557 - val_loss: 0.6437 - val_acc: 0.7868\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.6143 - acc: 0.8048 - val_loss: 0.5162 - val_acc: 0.8249\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4180 - acc: 0.8758 - val_loss: 0.3693 - val_acc: 0.8876\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3398 - acc: 0.8991 - val_loss: 0.3011 - val_acc: 0.9083\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3402 - acc: 0.8956 - val_loss: 0.2289 - val_acc: 0.9322\n",
      "Done inference\n",
      "\n",
      " Batch size: 8000 \tTrain time: 5.322608153025309e-06 \tInference time 6.637176239129269e-06 (11)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4146 - acc: 0.5482 - val_loss: 0.5582 - val_acc: 0.8554\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.6527 - acc: 0.7999 - val_loss: 0.3618 - val_acc: 0.9075\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4730 - acc: 0.8505 - val_loss: 0.4235 - val_acc: 0.8763\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3325 - acc: 0.9069 - val_loss: 0.2520 - val_acc: 0.9280\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3868 - acc: 0.8815 - val_loss: 0.2477 - val_acc: 0.9285\n",
      "Done inference\n",
      "\n",
      " Batch size: 9000 \tTrain time: 5.2799081802368165e-06 \tInference time 7.030390898386637e-06 (10)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4886 - acc: 0.5150 - val_loss: 0.7202 - val_acc: 0.7908\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 3us/step - loss: 0.7236 - acc: 0.7743 - val_loss: 0.4806 - val_acc: 0.8539\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 3us/step - loss: 0.4983 - acc: 0.8444 - val_loss: 0.3339 - val_acc: 0.9055\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.4044 - acc: 0.8757 - val_loss: 0.4669 - val_acc: 0.8374\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 4us/step - loss: 0.3529 - acc: 0.8916 - val_loss: 0.2945 - val_acc: 0.9137\n",
      "Done inference\n",
      "\n",
      " Batch size: 10000 \tTrain time: 5.695474942525228e-06 \tInference time 7.02138622601827e-06 (10)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST MLP\")\n",
    "print()\n",
    "mlp_cpu = MNIST_MLP('/cpu:0')\n",
    "mlp_gpu = MNIST_MLP('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally, the CNN implementation of MNIST will run out of memory. So we will create a backup now of train time and inference times for MLP MNIST in case this happens and we are forced to restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = open(\"backup.txt\", 'w')\n",
    "\n",
    "assert len(mlp_cpu.batch_sizes) == len(mlp_gpu.batch_sizes) == len(mlp_cpu.train_times) == len(mlp_cpu.inference_times) \\\n",
    "                 == len(mlp_gpu.train_times) == len(mlp_gpu.inference_times)\n",
    "for i in range(len(mlp_cpu.batch_sizes)):\n",
    "    assert mlp_cpu.batch_sizes[i] == mlp_gpu.batch_sizes[i]\n",
    "    backup.write(str(mlp_cpu.batch_sizes[i]) + '|' +\n",
    "                 str(mlp_cpu.train_times[i]) + '|' +\n",
    "                 str(mlp_cpu.inference_times[i]) + '|' +\n",
    "                 str(mlp_gpu.train_times[i]) + '|' +\n",
    "                 str(mlp_gpu.inference_times[i]) + '|' + '\\n')\n",
    "\n",
    "backup.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to get the data for the CNN implementations of MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_CNN\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-21a2aeb7a718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcnn_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f6f2c9eb7b2d>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0minference_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Batch size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\tTrain time:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\tInference time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(%s)'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0minference_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f6f2c9eb7b2d>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         history = self.model.fit(self.x_train, self.y_train, batch_size=batch_size, epochs=NUM_EPOCHS, verbose=1,\n\u001b[0;32m---> 44\u001b[0;31m                             validation_data=(self.x_test, self.y_test))\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;31m# update accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mnew_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;31m# use the new accumulator and the *old* delta_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(x, new_x)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \"\"\"\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    221\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    222\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    224\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     62\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                   use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_Flatten\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;34m\"\"\"Converts [1, 2, [3, 4], [5]] to [1, 2, 3, 4, 5].\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;31m# [1, 2, [3, 4], [5]] -> [[1], [2], [3, 4], [5]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0ml_of_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   \u001b[0;31m# [[1], [2], [3, 4], [5]] -> [1, 2, 3, 4, 5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_of_l\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;34m\"\"\"Converts [1, 2, [3, 4], [5]] to [1, 2, 3, 4, 5].\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;31m# [1, 2, [3, 4], [5]] -> [[1], [2], [3, 4], [5]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0ml_of_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   \u001b[0;31m# [[1], [2], [3, 4], [5]] -> [1, 2, 3, 4, 5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_of_l\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_IsListValue\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_IsListValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"MNIST_CNN\")\n",
    "cnn_cpu = MNIST_CNN('/cpu:0')\n",
    "cnn_gpu = MNIST_CNN('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"MLPdata.txt\", 'r')\n",
    "lines = f.read().split('\\n')\n",
    "f.close()\n",
    "mlp_cpu = MNIST_MLP(\"\\cpu:0\")\n",
    "mlp_gpu = MNIST_MLP(\"\\gpu:0\")\n",
    "\n",
    "first = False\n",
    "for line in lines:\n",
    "    if line == '': continue\n",
    "    batch_size, train_time, inference_time = line.split('|')\n",
    "    if batch_size == '1':\n",
    "        first = not first\n",
    "    if first:\n",
    "        mlp_cpu.batch_sizes.append(int(batch_size))\n",
    "        mlp_cpu.train_times.append(float(train_time))\n",
    "        mlp_cpu.inference_times.append(float(inference_time))\n",
    "    else:\n",
    "        mlp_cpu.batch_sizes.append(int(batch_size))\n",
    "        mlp_cpu.train_times.append(float(train_time))\n",
    "        mlp_cpu.inference_times.append(float(inference_time))\n",
    "        \n",
    "    \n",
    "cnn_cpu = MNIST_CNN(\"\\cpu:0\")\n",
    "f = open(\"CNNdata.txt\", 'r')\n",
    "lines = f.read().split('\\n')\n",
    "f.close()\n",
    "for line in lines:\n",
    "    if line == '': continue\n",
    "    batch_size, train_time, inference_time = line.split('|')\n",
    "    cnn_cpu.batch_sizes.append(int(batch_size))\n",
    "    cnn_cpu.train_times.append(float(train_time))\n",
    "    cnn_cpu.inference_times.append(float(inference_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Data\n",
    "Now we wish to compare train time and inference time between CPUs and GPUs. We will make two plots, one for train time and one for inference time. First, we import the required modules. Each plot will have both MLP and CNN on it, and they can be compared or viewed as separate. First we import the required modules.\n",
    "\n",
    "In general, MLP is plotted with cool colors and CNN is plotted with warm colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_mlp = np.array(mlp_cpu.batch_sizes)\n",
    "x_cnn = np.array(cnn_cpu.batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the data we have gathered and obtain a graph of train times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_mlp, mlp_cpu.train_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_mlp, mlp_gpu.train_times, c='r', alpha = 0.5, marker='s')\n",
    "#plt.scatter(x_cnn, cnn_cpu.train_times, c='y', alpha = 0.5, marker='^')\n",
    "#plt.scatter(x_cnn, cnn_gpu.train_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.001, 2])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a graph of inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_mlp, mlp_cpu.inference_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_mlp, mlp_gpu.inference_times, c='r', alpha = 0.5, marker='s')\n",
    "#plt.scatter(x_cnn, cnn_cpu.inference_times, c='y', alpha = 0.5, marker='^')\n",
    "#plt.scatter(x_cnn, cnn_gpu.inference_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.0003, 1])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a couple other plots, such as the performance gain in train time and inference time in using GPUs over CPUs. This is train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_improvement(cpu_times, gpu_times):\n",
    "    gain = []\n",
    "    for i in range(len(cpu_times)):\n",
    "        gain.append(cpu_times[i] / gpu_times[i] * 100)\n",
    "    return np.array(gain)\n",
    "\n",
    "gain_train_mlp = get_improvement(mlp_cpu.train_times, mlp_gpu.train_times)\n",
    "gain_train_cnn = get_improvement(cnn_cpu.train_times, cnn_gpu.train_times)\n",
    "\n",
    "plt.scatter(x_mlp, gain_train_mlp, c='k', alpha = 0.5, marker = 'd')\n",
    "plt.scatter(x_cnn, gain_train_cnn, c='c', alpha = 0.5, marker = '>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_inference_mlp = get_improvement(mlp_cpu.inference_times, mlp_gpu.inference_times)\n",
    "gain_inference_cnn = get_improvement(cnn_cpu.inference_times, cnn_gpu.inference_times)\n",
    "\n",
    "plt.scatter(x_mlp, gain_inference_mlp, c='k', alpha = 0.5, marker='d')\n",
    "plt.scatter(x_cnn, gain_inference_cnn, c='c', alpha = 0.5, marker='>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of the above data concludes this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
