{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the Train time and Inference Time of MNIST MLP and \n",
    "# CNN on CPUs and GPUs with `keras`\n",
    "\n",
    "The purpose of this notebook is to determine the relative speeds of training MNIST on CPUs and GPUs, as well as the relative speeds of inference. We will do this both for Convolutional Neural Network (CNN) and for Multi-Layer Perceptron (MLP) implementations of MNIST. It is similar to one of the experiments described in [this paper](https://arxiv.org/pdf/1904.08986.pdf) (arXiv:1904.08986v1 \\[physics.data-an\\]). The next step in this process will be to transform the code in this notebook into bare `tensorflow` code and compare the runtime between that and this `keras` implementation, and again between CPUs and GPUs in bare `tensorflow`. Then we will set the GPU implementation of MNIST up as a service. Ultimately, we look forward to running MNIST on TPUs and comparing runtime again.\n",
    "\n",
    "The code for the implementations of mnist using MLP with `keras` were pulled from [this github](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py), and the code for the CNN impementation was pulled from [this file](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) on the same github. I significantly changed both so that they would be more comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We import all the necessary classes and set some of the globals for the program. `NUM_CLASSES` is the number of categories to train mnist on. `NUM_EPOCHS` describes the number of epochs to run training over. `IMG_EDGE` is the side length of one of the (square) images, making the total pixel cound `IMG_EDGE ** 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adadelta\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Disable depreciation warnings\n",
    "try:\n",
    "    import tensorflow.python.util.deprecation as deprecation\n",
    "    deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "except AttributeError:\n",
    "    print(\"Import failed\")\n",
    "\n",
    "# Set global constants\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 5\n",
    "IMG_EDGE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating MNIST\n",
    "\n",
    "Having imported all the necessary modules and defined the constants, we need to implement MNIST in `keras`. Since we plan to implement MNIST with MLP and with CNN, we will first create a parent class holding the common functions of both. Later, we will define two subclasses, one for MLP and once for CNN. The parent class has several functions:\n",
    "- `_load` and `_finish_load` load the default `keras` MNIST dataset in whatever form MLP or CNN wants the data to be in.\n",
    "- `_get_batch_sizes` sets a list of all the batch sizes to test. Because CNN uses so much memory, we will need to test smaller batch sizes or else the machine will crash. However, for the MLP implementation, we can train on very high batch sizes. Hence the batch sizes will be different for each case.\n",
    "- `_create` creates the MNIST model with a specific batch size.\n",
    "- `_load_inferences` loads several randomly generated images to be inferred on.\n",
    "- `_train` trains the MNIST model, keeping track of the time it takes to do so, and returns that time. The time it returns is actually the time to train _per iteration_, where one iteration is the number of epochs times the number of data points divided by the batch size.\n",
    "- `_predict` runs a number of inferences equal to the batch size the model was trained on and returns the time per inference. It does this multiple times to reduce uncertainty.\n",
    "- `get_data` runs all of the above functions in order to get the train times and inference times for the given machine type and implementation (MLP or CNN) for all the batch sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, machine):\n",
    "        self.machine = machine\n",
    "        self.model = None\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
    "        self.start_power = 0\n",
    "        self.end_power = 0\n",
    "        \n",
    "    def _load(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _load_inferences(self, batch_size):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _get_batch_sizes(self):\n",
    "        self.batch_sizes = []\n",
    "        for i in range(self.start_power, self.end_power):\n",
    "            self.batch_sizes += list(range(10**i, 10**(i+1), 10**i))\n",
    "        self.batch_sizes += [10**self.end_power]\n",
    "    \n",
    "    def _finish_load(self):\n",
    "        self.x_train = self.x_train.astype('float32')\n",
    "        self.x_test = self.x_test.astype('float32')\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        print('Train dataset size:', self.x_train.shape[0])\n",
    "        print('Test dataset size:', self.x_test.shape[0])\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = to_categorical(self.y_train, NUM_CLASSES)\n",
    "        self.y_test = to_categorical(self.y_test, NUM_CLASSES)\n",
    "    \n",
    "    def _create(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _train(self, batch_size):\n",
    "        start_time = time()\n",
    "        history = self.model.fit(self.x_train, self.y_train, batch_size=batch_size, epochs=NUM_EPOCHS, verbose=1,\n",
    "                            validation_data=(self.x_test, self.y_test))\n",
    "        end_time = time()\n",
    "        iterations = NUM_EPOCHS * (self.x_train.shape[0] / batch_size)\n",
    "        train_time = (end_time - start_time) / iterations\n",
    "\n",
    "        #loss, accuracy = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        return train_time\n",
    "    \n",
    "    def _predict(self, batch_size):\n",
    "        inference_time = 0\n",
    "        start_inference = time()\n",
    "        inference_num = 0\n",
    "        while True: # Do multiple trials\n",
    "            inputs = self._load_inferences()\n",
    "\n",
    "            start_time = time()\n",
    "            self.model.predict(inputs, batch_size=batch_size)\n",
    "            end_time = time()\n",
    "            \n",
    "            inference_time += end_time - start_time\n",
    "            inference_num += 1\n",
    "            if end_time - start_inference > INFERENCE_TIME_THRESHOLD:\n",
    "                print(\"Done inference\")\n",
    "                break\n",
    "        iterations = inference_num * self.x_train.shape[0] / batch_size\n",
    "        return inference_time / iterations, inference_num\n",
    "    \n",
    "    def get_data(self):\n",
    "        self.train_times = []\n",
    "        self.inference_times = []\n",
    "        self.max_train = 0\n",
    "        self.max_inference = 0\n",
    "        \n",
    "        self._load()\n",
    "        self._get_batch_sizes()\n",
    "        for batch_size in self.batch_sizes:\n",
    "            self._create()\n",
    "            \n",
    "            train_time = self._train(batch_size)\n",
    "            inference_time, inference_num = self._predict(batch_size)\n",
    "            print('\\n','Batch size:', batch_size, '\\tTrain time:', train_time, '\\tInference time', inference_time, '(%s)'%inference_num)\n",
    "            print('+'*100)\n",
    "            self.train_times.append(train_time)\n",
    "            self.inference_times.append(inference_time)\n",
    "            \n",
    "            K.clear_session()# Clean up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our superclass, we may create two subclasses, one for MLP and one for CNN. They each handle the data and create the model differently, but in all other respects, `keras` allows us to treat them similarly, hence the superclass functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLP(MNIST):\n",
    "    def _load(self):\n",
    "        self.x_train = self.x_train.reshape(60000, IMG_EDGE**2)\n",
    "        self.x_test = self.x_test.reshape(10000, IMG_EDGE**2)\n",
    "        \n",
    "        self.start_power = 0\n",
    "        self.end_power = 4\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "    def _load_inferences(self):\n",
    "        return np.random.rand(self.x_train.shape[0], IMG_EDGE**2)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(512, activation='relu'))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=RMSprop(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(MNIST):\n",
    "    def _load(self):\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], 1, IMG_EDGE, IMG_EDGE)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], 1, IMG_EDGE, IMG_EDGE)\n",
    "            self.input_shape = (1, IMG_EDGE, IMG_EDGE)\n",
    "        else:\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "            self.input_shape = (IMG_EDGE, IMG_EDGE, 1)\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "        self.start_power = 0\n",
    "        self.end_power = 2# Smaller because CNN takes up more memory\n",
    "        \n",
    "    def _load_inferences(self, batch_size):\n",
    "        return np.random.rand(self.x_train.shape[0], IMG_EDGE, IMG_EDGE, 1)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "            self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(128, activation='relu'))\n",
    "            self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=Adadelta(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Now that we have defined all the methods we need to gather data on the train time and inference time of MNIST on different machines with different implementations, all we need to do is call the functions. `get_data` will generate our lists of train times and inference times for every batch size. This will take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"MNIST MLP\")\n",
    "print()\n",
    "mlp_cpu = MNIST_MLP('/cpu:0')\n",
    "mlp_gpu = MNIST_MLP('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally, the CNN implementation of MNIST will run out of memory. So we will create a backup now of train time and inference times for MLP MNIST in case this happens and we are forced to restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = open(\"backup.txt\", 'w')\n",
    "\n",
    "assert len(mlp_cpu.batch_sizes) == len(mlp_gpu.batch_sizes) == len(mlp_cpu.train_times) == len(mlp_cpu.inference_times) \\\n",
    "                 == len(mlp_gpu.train_times) == len(mlp_gpu.inference_times)\n",
    "for i in range(len(mlp_cpu.batch_sizes)):\n",
    "    assert mlp_cpu.batch_sizes[i] == mlp_gpu.batch_sizes[i]\n",
    "    backup.write(str(mlp_cpu.batch_sizes[i]) + '|' +\n",
    "                 str(mlp_cpu.train_times[i]) + '|' +\n",
    "                 str(mlp_cpu.inference_times[i]) + '|' +\n",
    "                 str(mlp_gpu.train_times[i]) + '|' +\n",
    "                 str(mlp_gpu.inference_times[i]) + '|' + '\\n')\n",
    "\n",
    "backup.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to get the data for the CNN implementations of MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MNIST_CNN\")\n",
    "cnn_cpu = MNIST_CNN('/cpu:0')\n",
    "cnn_gpu = MNIST_CNN('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Data\n",
    "Now we wish to compare train time and inference time between CPUs and GPUs. We will make two plots, one for train time and one for inference time. First, we import the required modules. Each plot will have both MLP and CNN on it, and they can be compared or viewed as separate. First we import the required modules.\n",
    "\n",
    "In general, MLP is plotted with cool colors and CNN is plotted with warm colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_mlp = np.array(mlp_cpu.batch_sizes)\n",
    "x_cnn = np.array(cnn_cpu.batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the data we have gathered and obtain a graph of train times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_mlp, mlp_cpu.train_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_mlp, mlp_gpu.train_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x_cnn, cnn_cpu.train_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.scatter(x_cnn, cnn_gpu.train_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.001, 2])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a graph of inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_mlp, mlp_cpu.inference_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_mlp, mlp_gpu.inference_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x_cnn, cnn_cpu.inference_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.scatter(x_cnn, cnn_gpu.inference_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.00001, 0.01])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a couple other plots, such as the performance gain in train time and inference time in using GPUs over CPUs. This is train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_improvement(cpu_times, gpu_times):\n",
    "    gain = []\n",
    "    for i in range(len(cpu_times)):\n",
    "        gain.append(cpu_times[i] / gpu_times[i] * 100)\n",
    "    return np.array(gain)\n",
    "\n",
    "gain_train_mlp = get_improvement(mlp_cpu.train_times, mlp_gpu.train_times)\n",
    "gain_train_cnn = get_improvement(cnn_cpu.train_times, cnn_gpu.train_times)\n",
    "\n",
    "plt.scatter(x_mlp, gain_train_mlp, c='k', alpha = 0.5, marker = 'd')\n",
    "plt.scatter(x_cnn, gain_train_cnn, c='c', alpha = 0.5, marker = '>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_inference_mlp = get_improvement(mlp_cpu.inference_times, mlp_gpu.inference_times)\n",
    "gain_inference_cnn = get_improvement(cnn_cpu.inference_times, cnn_gpu.inference_times)\n",
    "\n",
    "plt.scatter(x_mlp, gain_inference_mlp, c='k', alpha = 0.5, marker='d')\n",
    "plt.scatter(x_cnn, gain_inference_cnn, c='c', alpha = 0.5, marker='>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of the above data concludes this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
