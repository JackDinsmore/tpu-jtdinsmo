{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the Train time and Inference Time of MNIST MLP and \n",
    "# CNN on CPUs and GPUs with `keras`\n",
    "\n",
    "### Jack Dinsmore\n",
    "The purpose of this notebook is to determine the relative speeds of training MNIST on CPUs and GPUs, as well as the relative speeds of inference. We will do this both for CNN and for MLP implementations of MNIST. It is a mimicry of one of the experiments described in [this paper](https://arxiv.org/pdf/1904.08986.pdf) (arXiv:1904.08986v1 \\[physics.data-an\\]). Next, we will transform the code in this notebook into bare `tensorflow` code and compare runtime, set the GPU implementation up as a service, then transform the code again into a form that can be run on TPUs and perform a similar analysis.\n",
    "\n",
    "The code for the implementations of mnist using MLP with `keras` were pulled from [this github](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We import all the necessary classes and set some of the globals for the program. `INFERENCE_NUM` is the number of trials we will make when computing the time per inference of the model for each batch size for each machine; we average all the trial times together at the end. `NUM_CLASSES` is the number of categories to train mnist on. `NUM_EPOCHS` describes the number of epochs to run training over. `IMG_ROWS` and `IMG_COLS` will be the height and width respectively of the images. `BATCH_SIZES` is a list of sizes to over which compute runtimes, and it must be sorted from low to high in order for the graph to be made in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adadelta\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "INFERENCE_NUM = 1\n",
    "NUM_CLASSES=10\n",
    "NUM_EPOCHS=1\n",
    "IMG_ROWS=28\n",
    "IMG_COLS=28\n",
    "\n",
    "start_power = 0\n",
    "end_power = 4\n",
    "BATCH_SIZES = []\n",
    "for i in range(start_power, end_power):\n",
    "    BATCH_SIZES += list(range(10**i, 10**(i+1), 10**i))\n",
    "BATCH_SIZES += [10**end_power]\n",
    "BATCH_SIZES = [128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating MNIST\n",
    "\n",
    "Having imported all the necessary modules and defined the constants, we need to implement MNIST in `keras`. Since we plan to implement MNIST with MLP and with CNN, we will first create a parent class holding the common functions of both. Later, we will define two subclasses, one for MLP and once for CNN. The parent class has several functions:\n",
    "- `_load` and `_finish_load` load the default `keras` MNIST dataset in whatever form MLP or CNN wants the data to be in.\n",
    "- `_create` creates the MNIST model with a specific batch size.\n",
    "- `_load_inferences` loads several randomly generated images to be inferred on.\n",
    "- `_train` trains the MNIST model, keeping track of the time it takes to do so, and returns that time. The time it returns is actually the time to train _per iteration_, where one iteration is the number of epochs times the number of data points divided by the batch size.\n",
    "- `_predict` runs a number of inferences equal to the batch size the model was trained on and returns the time per inference. It does this multiple times to reduce uncertainty.\n",
    "- `get_data` runs all of the above functions in order to get the train times and inference times for the given machine type and implementation (MLP or CNN) for all the batch sizes in `BATCH_SIZE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, machine):\n",
    "        self.machine = machine\n",
    "        self.model = None\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
    "        \n",
    "    def _load(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _load_inferences(self, batch_size):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _finish_load(self):\n",
    "        self.x_train = self.x_train.astype('float32')\n",
    "        self.x_test = self.x_test.astype('float32')\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        print(self.x_train.shape[0], 'train samples')\n",
    "        print(self.x_test.shape[0], 'test samples')\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = to_categorical(self.y_train, NUM_CLASSES)\n",
    "        self.y_test = to_categorical(self.y_test, NUM_CLASSES)\n",
    "    \n",
    "    def _create(self):\n",
    "        # To be overrided\n",
    "        pass\n",
    "    \n",
    "    def _train(self, batch_size):\n",
    "        start_time = time()\n",
    "        history = self.model.fit(self.x_train, self.y_train, batch_size=batch_size, epochs=NUM_EPOCHS, verbose=1,\n",
    "                            validation_data=(self.x_test, self.y_test))\n",
    "        end_time = time()\n",
    "        iterations = NUM_EPOCHS * (self.x_train.shape[0] / batch_size)\n",
    "        train_time = (end_time - start_time) / iterations\n",
    "\n",
    "        #loss, accuracy = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        return train_time\n",
    "    \n",
    "    def _predict(self, batch_size):\n",
    "        inference_time = 0\n",
    "        for i in range(INFERENCE_NUM): # Do multiple trials\n",
    "            inputs = self._load_inferences(batch_size)\n",
    "\n",
    "            start_time = time()\n",
    "            self.model.predict(inputs)\n",
    "            end_time = time()\n",
    "            inference_time += end_time - start_time\n",
    "        inference_time /= INFERENCE_NUM\n",
    "\n",
    "        return inference_time / batch_size\n",
    "    \n",
    "    def get_data(self):\n",
    "        self.train_times = []\n",
    "        self.inference_times = []\n",
    "        self.max_train = 0\n",
    "        self.max_inference = 0\n",
    "        \n",
    "        self._load()\n",
    "        self._create()\n",
    "        for batch_size in BATCH_SIZES:\n",
    "            train_time = self._train(batch_size)\n",
    "            inference_time = self._predict(batch_size)\n",
    "            print('\\n','Batch size:', batch_size, '\\tTrain time:', train_time, '\\tInference time', inference_time)\n",
    "            print('+'*100)\n",
    "            self.train_times.append(train_time)\n",
    "            self.inference_times.append(inference_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our superclass, we may create two subclasses, one for MLP and one for CNN. They each handle the data and create the model differently, but in all other respects, `keras` allows us to treat them similarly, hence the superclass functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLP(MNIST):\n",
    "    def _load(self):\n",
    "        self.x_train = self.x_train.reshape(60000, IMG_ROWS*IMG_COLS)\n",
    "        self.x_test = self.x_test.reshape(10000, IMG_ROWS*IMG_COLS)\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "    def _load_inferences(self, batch_size):\n",
    "        return np.random.rand(batch_size, IMG_ROWS*IMG_COLS)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(512, activation='relu'))\n",
    "            self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=RMSprop(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(MNIST):\n",
    "    def _load(self):\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], 1, IMG_ROWS, IMG_COLS)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], 1, IMG_ROWS, IMG_COLS)\n",
    "            self.input_shape = (1, IMG_ROWS, IMG_COLS)\n",
    "        else:\n",
    "            self.x_train = self.x_train.reshape(self.x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "            self.x_test = self.x_test.reshape(self.x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "            self.input_shape = (IMG_ROWS, IMG_COLS, 1)\n",
    "        \n",
    "        self._finish_load()\n",
    "        \n",
    "    def _load_inferences(self, batch_size):\n",
    "        return np.random.rand(batch_size, IMG_ROWS, IMG_COLS, 1)\n",
    "    \n",
    "    def _create(self):\n",
    "        with tf.device(self.machine):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "            self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(128, activation='relu'))\n",
    "            self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            self.model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=Adadelta(),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Now that we have defined all the methods we need to gather data on the train time and inference time of MNIST on different machines with different implementations, all we need to do is call the functions. `get_data` will generate our lists of train times and inference times for every batch size. This will take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST MLP\n",
      "\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "(60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.2463 - acc: 0.9249 - val_loss: 0.1020 - val_acc: 0.9674\n",
      "\n",
      " Batch size: 128 \tTrain time: 0.025832924397786457 \tInference time 0.0027866456657648087\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "TRAIN ON GPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "(60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2459 - acc: 0.9241 - val_loss: 0.1054 - val_acc: 0.9675\n",
      "\n",
      " Batch size: 128 \tTrain time: 0.010461168924967448 \tInference time 0.0031428560614585876\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST MLP\")\n",
    "print()\n",
    "mlp_cpu = MNIST_MLP('/cpu:0')\n",
    "mlp_gpu = MNIST_MLP('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may do the same for the CNN implementations of MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_CNN\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.2535 - acc: 0.9223 - val_loss: 0.0586 - val_acc: 0.9807\n",
      "\n",
      " Batch size: 128 \tTrain time: 0.5003279586791992 \tInference time 0.004216257482767105\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "TRAIN ON GPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST_CNN\")\n",
    "cnn_cpu = MNIST_CNN('/cpu:0')\n",
    "cnn_gpu = MNIST_CNN('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_cpu.get_data()\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Data\n",
    "Now we wish to compare train time and inference time between CPUs and GPUs. We will make two plots, one for train time and one for inference time. First, we import the required modules. Each plot will have both MLP and CNN on it, and they can be compared or viewed as separate. First we import the required modules.\n",
    "\n",
    "In general, MLP is plotted with cool colors and CNN is plotted with warm colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array(BATCH_SIZES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the data we have gathered and obtain a graph of train times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, mlp_cpu.train_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x, mlp_gpu.train_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x, cnn_cpu.train_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.scatter(x, cnn_gpu.train_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a graph of inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, mlp_cpu.inference_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x, mlp_gpu.inference_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x, cnn_cpu.inference_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.scatter(x, cnn_gpu.inference_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a couple other plots, such as the performance gain in train time and inference time in using GPUs over CPUs. This is train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_improvement(cpu_times, gpu_times):\n",
    "    gain = []\n",
    "    for i in range(cpu_times.shape[0]):\n",
    "        gain.append(cpu_times[i] / gpu_times[i] * 100)\n",
    "    return np.array(gain)\n",
    "\n",
    "gain_train_mlp = get_improvement(mlp_cpu.train_times, mlp_gpu.train_times)\n",
    "gain_train_cnn = get_improvement(cnn_cpu.train_times, cnn_gpu.train_times)\n",
    "\n",
    "plt.scatter(x, gain_train_mlp, c='k', alpha = 0.5, marker = 'd')\n",
    "plt.scatter(x, gain_train_cnn, c='c', alpha = 0.5, marker = '>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_inference_mlp = get_improvement(mlp_cpu.inference_times, mlp_gpu.inference_times)\n",
    "gain_inference_cnn = get_improvement(cnn_cpu.inference_times, cnn_gpu.inference_times)\n",
    "\n",
    "plt.scatter(x, gain_inference_mlp, c='k', alpha = 0.5, marker='d')\n",
    "plt.scatter(x, gain_inference_cnn, c='c', alpha = 0.5, marker='>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of the above data concludes this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
