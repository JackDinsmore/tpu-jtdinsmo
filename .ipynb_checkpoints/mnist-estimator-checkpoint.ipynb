{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with `Estimator`\n",
    "The purpose of this notebook is to convert the `keras` implementation of MNIST MLP in the notebook cpu-gpu-mnist into an `Estimator` implementation, which will be faster and will be an important step along the way to implementing MNIST for TPUs.\n",
    "\n",
    "I'm getting a lot of my code from [this](https://towardsdatascience.com/first-contact-with-tensorflow-estimator-69a5e072998d) tutorial on estimators, plus some extra code on inferences from [this](https://medium.com/@vincentteyssier/tensorflow-estimator-tutorial-on-real-life-data-aa0fca773bb) blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Disable depreciation warnings and limit verbosity during training\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "tf.logging.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create some global variables which will define the learning process. Those in the first bock are set to be identical to the corresponding global variables in the `keras` implementation of MNIST in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_CHANNELS = 1\n",
    "NUM_EPOCHS = 1\n",
    "IMG_EDGE = 28\n",
    "\n",
    "start_power = 0\n",
    "end_power = 4\n",
    "BATCH_SIZES = []\n",
    "for i in range(start_power, end_power):\n",
    "    BATCH_SIZES += list(range(10**i, 10**(i+1), 10**i))\n",
    "BATCH_SIZES += [10**end_power]\n",
    "\n",
    "BATCH_SIZES = [10, 100, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must download the MNIST dataset. We will download it in its orginal form, fitted neither to MLP format nor CNN format. We will reformat it for each of these implementations later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Train dataset size: 55000\n",
      "Eval dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images  # Returns a np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns a np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "print()\n",
    "print(\"Train dataset size:\", train_data.shape[0])\n",
    "print(\"Eval dataset size:\", eval_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to describe the behavior of the `Estimator` during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=eval_data,\n",
    "        y=eval_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "Now we will create a superclass &mdash; much like we did in the `keras` implementation of MNIST in this repository &mdash; which contains all of the functions common to both MLP and CNN implementations of MNIST. The methods of this superclass are described as follows:\n",
    "- `__init__` simply sets the device to use, either CPU or GPU so that it is consistent throughout the whole process of training and predicting on the `Estimator`.\n",
    "- `_load` perfoms all the implementation-specific functions, such as determining the topology of the neural net, and reshaping the data.\n",
    "- `model_fn` is a required argument of `tf.estimator.Estimator.__init__` which describes the behavior of the `Estimator`. \n",
    "- `_train` will train the `Estimator` and return the time per iteration it takes to do so, just as it did in the `keras` implementation of MNIST in this repository.\n",
    "- `_predict` does the same but for inference. It will return the time it takes to do inference on the `Estimator` per inference, where the batch size refers to the number of inferences to do in parallel.\n",
    "- `_main` is the function that the `tensorflow` app runs, which calls `_train` and `_predict` and gathers all the data into `self.train_times` and `self.inference_times`.\n",
    "- `get_data` is the only \"public\" function &mdash; the only one which is called outside of the class. It calls `_main`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_train_data = []\n",
    "batch_inference_data = []\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.learning_rate = None\n",
    "    \n",
    "    # To be overrided\n",
    "    def _load(self, features):\n",
    "        return None, None\n",
    "        \n",
    "    def model_fn(self, features, labels, mode):\n",
    "        '''\n",
    "        Arguments:\n",
    "            features: items returned from input_fn\n",
    "            labels: second item returned from input_fn\n",
    "            mode: the mode the estimator is running in (basically training, validation or prediction). Either\n",
    "                - tf.estimator.ModeKeys.TRAIN\n",
    "                - tf.estimator.ModeKeys.EVAL\n",
    "                - tf.estimator.ModeKeys.PREDICT\n",
    "        Returns:\n",
    "            tf.estimator.EstimatorSpec containing\n",
    "                - the train/loss ops for training\n",
    "                - loss and metrics for evaluation\n",
    "                - predictions for inference\n",
    "        '''\n",
    "\n",
    "        global_step=tf.train.get_global_step()\n",
    "        \n",
    "        model, images = self._load(features)\n",
    "        if model == None:\n",
    "            raise Exception(\"The class MNIST is not meant to be instantiated.\")\n",
    "\n",
    "        logits = model(images)\n",
    "        predicted_logit = tf.argmax(input=logits, axis=1, \n",
    "                                    output_type=tf.int32)\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "        #PREDICT\n",
    "        predictions = {\n",
    "          \"predicted_logit\": predicted_logit,\n",
    "          \"probabilities\": probabilities\n",
    "        }\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode,  \n",
    "                                            predictions=predictions)\n",
    "        with tf.name_scope('loss'):\n",
    "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "                    labels=labels, logits=logits, scope='loss')\n",
    "            tf.summary.scalar('loss', cross_entropy)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.metrics.accuracy(\n",
    "                    labels=labels, predictions=predicted_logit, name='acc')\n",
    "            tf.summary.scalar('accuracy', accuracy[1])\n",
    "            #EVAL\n",
    "            if mode == tf.estimator.ModeKeys.EVAL:\n",
    "                return tf.estimator.EstimatorSpec(\n",
    "                    mode=mode,\n",
    "                    loss=cross_entropy,\n",
    "                    eval_metric_ops={'accuracy/accuracy': accuracy},\n",
    "                    evaluation_hooks=None)\n",
    "\n",
    "\n",
    "        # Create a SGR optimizer \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        train_op = optimizer.minimize( \n",
    "                    cross_entropy,global_step=global_step)\n",
    "\n",
    "        # Create a hook to print acc, loss & global step every 100 iter.   \n",
    "        train_hook_list= []\n",
    "        train_tensors_log = {'accuracy': accuracy[1],\n",
    "                             'loss': cross_entropy,\n",
    "                             'global_step': global_step}\n",
    "        train_hook_list.append(tf.train.LoggingTensorHook(\n",
    "            tensors=train_tensors_log, every_n_iter=100))\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "              mode=mode,\n",
    "              loss=cross_entropy,\n",
    "              train_op=train_op,\n",
    "              training_hooks=train_hook_list)\n",
    "\n",
    "    \n",
    "    def _train(self, batch_size):\n",
    "        # train_input_fn must be redefined for every batch size.\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x=train_data,\n",
    "            y=train_labels,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=1,\n",
    "            shuffle=True)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i in range(NUM_EPOCHS):\n",
    "            epoch_start_time = time.time()\n",
    "            print(\"Epoch \" + str(i+1) + '/' + str(NUM_EPOCHS)+': ', end = '')\n",
    "            self.image_classifier.train(input_fn=train_input_fn)\n",
    "            metrics = self.image_classifier.evaluate(input_fn=eval_input_fn)\n",
    "            epoch_end_time = time.time()\n",
    "            print('Accuracy:', metrics['accuracy/accuracy'], '\\tLoss:', metrics['loss'],\n",
    "                  '\\tTime:', epoch_end_time - epoch_start_time, 's')\n",
    "        end_time = time.time()\n",
    "        iterations = NUM_EPOCHS * (train_data.shape[0] / batch_size)\n",
    "        train_time = (end_time - start_time) / iterations\n",
    "        return train_time\n",
    "    \n",
    "    def _predict(self, batch_size):\n",
    "        inference_time = 0\n",
    "        inference_num = 0\n",
    "        start_inference = time.time()\n",
    "        while True:\n",
    "            # Remade every iteration to randomize\n",
    "            predict_data = np.random.rand(batch_size, IMG_EDGE**2).astype('float32')\n",
    "            predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                x=predict_data,\n",
    "                y=None,\n",
    "                batch_size=batch_size,\n",
    "                num_epochs=1,\n",
    "                shuffle=True)\n",
    "        \n",
    "            start_time = time.time()\n",
    "            preds = self.image_classifier.predict(predict_input_fn)\n",
    "            for i in range(batch_size):\n",
    "                s = next(preds)\n",
    "            end_time = time.time()\n",
    "            inference_time += end_time - start_time\n",
    "            inference_num += 1\n",
    "            if end_time - start_inference > 5:# Don't let inference take longer than 5 seconds\n",
    "                break\n",
    "        inference_time = inference_time / inference_num / batch_size\n",
    "        return inference_time, inference_num\n",
    "    \n",
    "    def _main(self, _):\n",
    "        self.train_times = []\n",
    "        self.inference_times = []\n",
    "        with tf.device(self.device):\n",
    "            self.image_classifier = tf.estimator.Estimator(model_fn=self.model_fn, model_dir=None)\n",
    "                # set model_dir to make model progress permanent\n",
    "            for batch_size in BATCH_SIZES:\n",
    "                train_time = self._train(batch_size)\n",
    "                inference_time, inference_num = self._predict(batch_size)\n",
    "                print('\\n','Batch size:', batch_size, '\\tTrain time:', train_time, '\\tInference time', inference_time, '(%s)'%inference_num)\n",
    "                print('+'*100)\n",
    "                self.train_times.append(train_time)\n",
    "                self.inference_times.append(inference_time)\n",
    "            \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            tf.app.run(self._main)\n",
    "        except SystemExit:\n",
    "            # Prevent the program from exiting when done\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the superclass defined, we must now implement it for MLP and CNN versions of MNIST. For each, we define a class which contains the topology of the MNIST neural net (e.g., `ModelMLP`), and then we implement a subclass of the class `MNIST` we just defined which returns it (e.g., `MNIST_MLP`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMLP(object):\n",
    "    def __call__(self, inputs):\n",
    "        net = tf.layers.dense(inputs, 512, activation=tf.nn.relu, name='dense1')\n",
    "        net = tf.layers.dropout(net, rate=0.2, name='drop1')\n",
    "        net = tf.layers.dense(net, 512, activation=tf.nn.relu, name='dense2')\n",
    "        net = tf.layers.dropout(net, rate=0.2, name='drop2')\n",
    "        net = tf.layers.dense(net, NUM_CLASSES, activation=tf.nn.softmax, name='dense3')\n",
    "        return net\n",
    "    \n",
    "class MNIST_MLP(MNIST):\n",
    "    def _load(self, features):\n",
    "        images = tf.reshape(features, [-1, IMG_EDGE**2])\n",
    "        self.learning_rate = 0.001# Learning rate from keras.optimizers.RMSprop\n",
    "        return ModelMLP(), images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN(object):\n",
    "    def __call__(self, inputs):\n",
    "        net = tf.layers.conv2d(inputs, 32, [5, 5], activation=tf.nn.relu, name='conv1')\n",
    "        net = tf.layers.max_pooling2d(net, [2, 2], 2, name='pool1')\n",
    "        net = tf.layers.conv2d(net, 64, [5, 5], activation=tf.nn.relu, name='conv2')\n",
    "        net = tf.layers.max_pooling2d(net, [2, 2], 2, name='pool2')\n",
    "        net = tf.layers.flatten(net)\n",
    "        net = tf.layers.dense(net, NUM_CLASSES, activation=None, name='fc1')\n",
    "        return net\n",
    "          \n",
    "class MNIST_CNN(MNIST):\n",
    "    def _load(self, features):\n",
    "        images = tf.reshape(features, [-1, IMG_EDGE, IMG_EDGE, NUM_CHANNELS])\n",
    "        self.learning_rate = 1.0# Learning rate from keras.optimizers.Adadelta\n",
    "        return ModelCNN(), images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the subclasses implemented, all we need to do is instantiate them and call `get_data` for both. This will train all the models and run all the inferences for all the batch sizes we need, with all possible combinations of machine type (CPU or GPU) and MNIST implementation (MLP or CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST MLP\n",
      "\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\goods\\AppData\\Local\\Temp\\tmp_87fr6kb\n",
      "Epoch 1/1: Accuracy: 0.2794 \tLoss: 2.2016447 \tTime: 49.32428503036499 s\n",
      "\n",
      " Batch size: 10 \tTrain time: 0.008968595721504906 \tInference time 0.08093748092651368\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch 1/1: Accuracy: 0.3029 \tLoss: 2.1858287 \tTime: 13.641675472259521 s\n",
      "\n",
      " Batch size: 100 \tTrain time: 0.024813929037614302 \tInference time 0.008763542175292969\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch 1/1: Accuracy: 0.3062 \tLoss: 2.1841934 \tTime: 9.952755451202393 s\n",
      "\n",
      " Batch size: 1000 \tTrain time: 0.18099546432495117 \tInference time 0.0008168199062347412\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "TRAIN ON GPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\goods\\AppData\\Local\\Temp\\tmprjaokcao\n",
      "Epoch 1/1: Accuracy: 0.3958 \tLoss: 2.2054248 \tTime: 53.81928777694702 s\n",
      "\n",
      " Batch size: 10 \tTrain time: 0.00978586920824918 \tInference time 0.11279261112213135\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch 1/1: Accuracy: 0.4166 \tLoss: 2.1862643 \tTime: 14.638141393661499 s\n",
      "\n",
      " Batch size: 100 \tTrain time: 0.0266166166825728 \tInference time 0.006662185192108154\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch 1/1: Accuracy: 0.4186 \tLoss: 2.1841657 \tTime: 7.72667932510376 s\n",
      "\n",
      " Batch size: 1000 \tTrain time: 0.1405031984502619 \tInference time 0.0008823919296264648\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST MLP\")\n",
    "print()\n",
    "mlp_cpu = MNIST_MLP('/cpu:0')\n",
    "mlp_gpu = MNIST_MLP('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_cpu.get_data()\n",
    " \n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "mlp_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST CNN\n",
      "\n",
      "\n",
      "TRAIN ON CPUS\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\goods\\AppData\\Local\\Temp\\tmp85uzjny4\n",
      "Epoch 1/1: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-1e5c66b486c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'+'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcnn_cpu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-4cbb322ec2ad>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_main\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Prevent the program from exiting when done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-4cbb322ec2ad>\u001b[0m in \u001b[0;36m_main\u001b[1;34m(self, _)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[1;31m# set model_dir to make model progress permanent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mBATCH_SIZES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mtrain_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[0minference_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Batch size:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\tTrain time:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\tInference time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-4cbb322ec2ad>\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[0mepoch_end_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             print('Accuracy:', metrics['accuracy/accuracy'], '\\tLoss:', metrics['loss'],\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    467\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m   def _actual_eval(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[1;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0meval_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[0mall_hooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m             output_dir=self.eval_dir(name))\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_evaluate_run\u001b[1;34m(self, checkpoint_path, scaffold, update_op, eval_dict, all_hooks, output_dir)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         \u001b[0mfinal_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1537\u001b[1;33m         config=self._session_config)\n\u001b[0m\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m     \u001b[0mcurrent_global_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGLOBAL_STEP\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[1;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1172\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1253\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"MNIST CNN\")\n",
    "print()\n",
    "cnn_cpu = MNIST_CNN('/cpu:0')\n",
    "cnn_gpu = MNIST_CNN('/gpu:0')\n",
    "\n",
    "print()\n",
    "print(\"TRAIN ON CPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_cpu.get_data()\n",
    " \n",
    "print()\n",
    "print(\"TRAIN ON GPUS\")\n",
    "print()\n",
    "print('+'*100)\n",
    "cnn_gpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing Data\n",
    "Now we will graph the data we have collected above. First we import the `matplotlib` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array(BATCH_SIZES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the train time and inference times on CPUs and GPUs as a function of batch size. This is train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, mlp_cpu.train_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x, mlp_gpu.train_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x, cnn_cpu.train_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.scatter(x, cnn_gpu.train_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.001, 2])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inference time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, mlp_cpu.inference_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x, mlp_gpu.inference_times, c='r', alpha = 0.5, marker='s')\n",
    "plt.scatter(x, cnn_cpu.inference_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.scatter(x, cnn_gpu.inference_times, c='m', alpha = 0.5, marker='v')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.0001, 0.1])\n",
    "plt.legend(['MLP CPU', 'MLP GPU', 'CNN CPU', 'CNN GPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the performance gain in using GPUs over CPUs. This is performance gain in train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_improvement(cpu_times, gpu_times):\n",
    "    gain = []\n",
    "    for i in range(len(cpu_times)):\n",
    "        gain.append(cpu_times[i] / gpu_times[i] * 100)\n",
    "    return np.array(gain)\n",
    "\n",
    "gain_train_mlp = get_improvement(mlp_cpu.train_times, mlp_gpu.train_times)\n",
    "gain_train_cnn = get_improvement(cnn_cpu.train_times, cnn_gpu.train_times)\n",
    "\n",
    "plt.scatter(x, gain_train_mlp, c='k', alpha = 0.5, marker = 'd')\n",
    "plt.scatter(x, gain_train_cnn, c='c', alpha = 0.5, marker = '>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the performance gain in inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_inference_mlp = get_improvement(mlp_cpu.inference_times, mlp_gpu.inference_times)\n",
    "gain_inference_cnn = get_improvement(cnn_cpu.inference_times, cnn_gpu.inference_times)\n",
    "\n",
    "plt.scatter(x, gain_inference_mlp, c='k', alpha = 0.5, marker='d')\n",
    "plt.scatter(x, gain_inference_cnn, c='c', alpha = 0.5, marker='>')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference speed gain by using GPUs (%)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('linear')\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.axhline(100, linestyle='--', linewidth=1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
