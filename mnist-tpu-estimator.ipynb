{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with `TPUEstimator`\n",
    "The purpose of this notebook is to convert the `Estimator` implementation of MNIST MLP in the notebook mnist-estimator.ipynb into an `TPUEstimator` implementation. With this done, we will have completed our ultimate quest to compare the runtime of MNIST on CPUs, GPUs, and TPUs, with the same code and datasets.\n",
    "\n",
    "Most of this code will be ported from the last notebook, mnist-estimator.ipynb. In order to port it, we will make use of [this](https://cloud.google.com/tpu/docs/tutorials/migrating-to-tpuestimator-api) Google tutorial on how to port `Estimator` to `TPUEstimator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Disable depreciation warnings and limit verbosity during training\n",
    "try:\n",
    "    from tensorflow.python.util import deprecation\n",
    "    deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "except AttributeError:\n",
    "    print(\"Import of warning suppression module failed\")\n",
    "    \n",
    "tf.logging.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create some global variables which will define the learning process. They are set to be identical to the corresponding global variables in the `keras` implementation of MNIST in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 1\n",
    "IMG_EDGE = 28\n",
    "MODEL_DIR = 'gs://harrisgroup-ctpu/jtdinsmo/mnist/output/'\n",
    "DATA_DIR = 'gs://harrisgroup-ctpu/jtdinsmo/mnist/data/'\n",
    "TPU_NAME='jtdinsmo-tpu-2'\n",
    "ZONE_NAME='us-central1-b'\n",
    "PROJECT_NAME = 'harrisgroup-223921'\n",
    "NUM_ITERATIONS = 50 # Number of iterations per TPU training loop\n",
    "TRAIN_STEPS = 1000\n",
    "EVALUATE_STEPS = 1000\n",
    "INFERENCE_TIME_THRESHOLD = 10 # Seconds\n",
    "NUM_SHARDS = 8 # Number of shards (TPU chips).\n",
    "LEARNING_RATE = 0.05\n",
    "DEBUG = True # Set to True to prevent the TPU cluster from restarting, but this affects\n",
    "    # train time, possibly inference time, and creates unusual errors such as \n",
    "    # UNHEALTHY_TENSORFLOW and sometimes it kills the kernel inexplicably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must download the MNIST dataset. We will download it in its orginal form, fitted neither to MLP format nor CNN format. We will reformat it for each of these implementations later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images  # Returns an np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns an np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to describe the behavior of the `TPUEstimator` during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(labels, logits):\n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        labels=labels, predictions=tf.argmax(logits, axis=1))\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "def eval_input_fn(params):\n",
    "    return (eval_data, eval_labels)\n",
    "    \n",
    "def train_input_fn(params):\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    train_data_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "    train_labels_dataset = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "    dataset_train = tf.data.Dataset.zip((train_data_dataset, train_labels_dataset))\n",
    "    return dataset_train.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "    #return(train_data, train_labels)\n",
    "\n",
    "def purge_model_files():\n",
    "    '''Used at the beginning of the code that runs MNIST for CNN and MLP implementations.\n",
    "    If one of these two implementations is run after the other, it will throw hard-to-\n",
    "    debug errors about how checkpoint files have been changed. All that is really happening\n",
    "    is the program is trying to load checkpoints made with the previous model.'''\n",
    "    \n",
    "    if os.system(\"gsutil rm -r %s\"%(MODEL_DIR)):\n",
    "        print(\"Model directory could not be purged.\")\n",
    "    else:\n",
    "        print(\"Model directory purged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "Now we will create a superclass &mdash; much like we did in the `keras` implementation of MNIST in this repository &mdash; which contains all of the functions common to both MLP and CNN implementations of MNIST. The methods of this superclass are described as follows:\n",
    "- `_load` perfoms all the implementation-specific functions, such as determining the topology of the neural net, and reshaping the data.\n",
    "- `model_fn` is a required argument of `tf.contrib.tpu.TPUEstimator.__init__` which describes the behavior of the `TPUEstimator`. \n",
    "- `_create` creates the `TPUEstimator`. Sometimes, it throws an `UNHEALTHY_TENSORFLOW` error if several models have already been created on the TPU cluster. In this case, stopping and restarting the cluster will solve the issue. We therefore stop and restart the TPU cluster every time `_create` is called, even when it doesn't throw the `UNHEALTHY_TENSORFLOW` for the sake of parity.\n",
    "- `_train` will train the `TPUEstimator` and return the time per iteration it takes to do so, just as it did in the `keras` implementation of MNIST in this repository.\n",
    "- `_predict` does the same but for inference. It will return the time it takes to do inference on the `TPUEstimator` per inference, where the batch size refers to the number of inferences to do in parallel.\n",
    "- `_main` is the function that the `tensorflow` app runs, which calls `_train` and `_predict` and gathers all the data into `self.train_times` and `self.inference_times`.\n",
    "- `get_data` is the only \"public\" function &mdash; the only one which is called outside of the class. It calls `_main`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_train_data = []\n",
    "batch_inference_data = []\n",
    "MAX_STEPS = train_data.shape[0] // 8 + 1\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = None\n",
    "        self.use_tpu = True\n",
    "    \n",
    "    # To be overridden\n",
    "    def _load(self, features):\n",
    "        '''\n",
    "        Returns the model, the new features formatted in the way\n",
    "        MLP or CNN needs them.\n",
    "        '''\n",
    "        return None, None\n",
    "    \n",
    "    # To be overridden\n",
    "    def _load_batch_size_limits(self):\n",
    "        self.start_power = None\n",
    "        self.end_power = None\n",
    "    \n",
    "    def _get_batch_sizes(self):\n",
    "        self._load_batch_size_limits()\n",
    "        self.batch_sizes = []\n",
    "        for i in range(self.start_power, self.end_power):\n",
    "            self.batch_sizes += list(range(8**i, 8**(i+1), 8**i))\n",
    "        self.batch_sizes += [8**self.end_power]\n",
    "        self.batch_sizes = [128]\n",
    "        \n",
    "    def model_fn(self, features, labels, mode, params):\n",
    "        del params# Unused\n",
    "        image = features\n",
    "        if isinstance(image, dict):\n",
    "            image = features[\"image\"]\n",
    "        model, image = self._load(image)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            logits = model(image)\n",
    "            predictions = {\n",
    "                'class_ids': tf.argmax(logits, axis=1),\n",
    "                'probabilities': tf.nn.softmax(logits),\n",
    "            }\n",
    "            return tf.contrib.tpu.TPUEstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "        logits = model(image)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                LEARNING_RATE,\n",
    "                tf.train.get_global_step(),\n",
    "                decay_steps=100000,\n",
    "                decay_rate=0.96)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            if self.use_tpu:\n",
    "                optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "            return tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                train_op=optimizer.minimize(loss, tf.train.get_global_step()))\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode, loss=loss, eval_metrics=(metric_fn, [labels, logits]))\n",
    "\n",
    "    def _train(self, batch_size):\n",
    "        start_time = time.time()\n",
    "        for i in range(NUM_EPOCHS):\n",
    "            epoch_start_time = time.time()\n",
    "            print(\"Epoch \" + str(i+1) + '/' + str(NUM_EPOCHS)+': ', end='')\n",
    "            \n",
    "            try:\n",
    "                self.estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            \n",
    "            metrics = self.estimator.evaluate(input_fn=eval_input_fn, steps=EVALUATE_STEPS)\n",
    "            epoch_end_time = time.time()\n",
    "            print('Accuracy:', metrics['accuracy'], '\\tLoss:', metrics['loss'],\n",
    "                  '\\tTime:', epoch_end_time - epoch_start_time, 's')\n",
    "        end_time = time.time()\n",
    "        iterations = NUM_EPOCHS * (train_data.shape[0] / batch_size)\n",
    "        train_time = (end_time - start_time) / iterations\n",
    "        return train_time\n",
    "    \n",
    "    def _predict(self, batch_size):\n",
    "        inference_time = 0\n",
    "        inference_num = 0\n",
    "        start_inference = time.time()\n",
    "        while True:\n",
    "            random_dataset = np.random.rand(train_data.shape[0], IMG_EDGE**2).astype('float32')\n",
    "            dataset_predict = tf.data.Dataset.from_tensor_slices(random_dataset)\n",
    "            \n",
    "            def predict_input_fn(params):\n",
    "                batch_size = params[\"batch_size\"]\n",
    "                dataset_predict = dataset_predict.batch(batch_size)\n",
    "                return dataset_predict\n",
    "\n",
    "            start_time = time.time()\n",
    "            predictions = self.estimator.predict(predict_input_fn)\n",
    "            \n",
    "            #for pred_dict in predictions:\n",
    "            #    class_id = pred_dict['class_ids']\n",
    "            #    probability = pred_dict['probabilities'][class_id]\n",
    "                #print(\"Prediction is\", class_id, \"(\"+str(100 * probability)+\").\")\n",
    "                \n",
    "            end_time = time.time()\n",
    "            inference_time += end_time - start_time\n",
    "            inference_num += 1\n",
    "            if end_time - start_inference > INFERENCE_TIME_THRESHOLD:\n",
    "                # Do as many inferences as possible in INFERENCE_TIME_THRESHOLD seconds\n",
    "                break\n",
    "        inference_time = inference_time / inference_num / batch_size\n",
    "        return inference_time, inference_num\n",
    "    \n",
    "    def _create(self, batch_size):\n",
    "        # Restart the TPU cluster to avoid UNHEALTHY_TENSORFLOW\n",
    "        if not DEBUG:# Don't restart on the first run\n",
    "            print(\"Restarting TPU cluster, \", end='')\n",
    "            os.system(\"gcloud compute tpus stop jtdinsmo-tpu-2 --zone us-central1-b\")\n",
    "            os.system(\"gcloud compute tpus start jtdinsmo-tpu-2 --zone us-central1-b\")\n",
    "            time.sleep(15)\n",
    "                # Pause for a bit to allow the TPU clusters to finish running \n",
    "                # startup scripts if there are any.\n",
    "            print(\"TPU cluster restarted\")\n",
    "\n",
    "        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "            TPU_NAME,\n",
    "            zone=ZONE_NAME,\n",
    "            project=PROJECT_NAME)\n",
    "\n",
    "        run_config = tf.contrib.tpu.RunConfig(\n",
    "            cluster=tpu_cluster_resolver,\n",
    "            model_dir=MODEL_DIR,\n",
    "            session_config=tf.ConfigProto(\n",
    "            allow_soft_placement=True, log_device_placement=True),\n",
    "            tpu_config=tf.contrib.tpu.TPUConfig(NUM_ITERATIONS, NUM_SHARDS),)\n",
    "        \n",
    "        self.estimator = tf.contrib.tpu.TPUEstimator(\n",
    "            model_fn=self.model_fn,\n",
    "            use_tpu=self.use_tpu,\n",
    "            train_batch_size=batch_size,\n",
    "            eval_batch_size=batch_size,\n",
    "            predict_batch_size=batch_size,\n",
    "            params={\"data_dir\": DATA_DIR},\n",
    "            config=run_config)\n",
    "    \n",
    "    def _main(self, _):\n",
    "        self.train_times = []\n",
    "        self.inference_times = []\n",
    "        self._get_batch_sizes()\n",
    "        for batch_size in self.batch_sizes:\n",
    "            self._create(batch_size)\n",
    "                            \n",
    "            train_time = self._train(batch_size)\n",
    "                \n",
    "            inference_time, inference_num = self._predict(batch_size)\n",
    "            print('\\n','Batch size:', batch_size, '\\tTrain time:', train_time,\n",
    "                  '\\tInference time', inference_time, '(%s)'%inference_num)\n",
    "            print('+'*100)\n",
    "            self.train_times.append(train_time)\n",
    "            self.inference_times.append(inference_time)\n",
    "            \n",
    "            tf.reset_default_graph()# For memory conservation\n",
    "            \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            tf.app.run(self._main)\n",
    "        except SystemExit:\n",
    "            # Prevent the program from exiting when done\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the superclass defined, we must now implement it for MLP and CNN versions of MNIST. For each, we define a class which contains the topology of the MNIST neural net (e.g., `ModelMLP`), and then we implement a subclass of the class `MNIST` we just defined which returns it (e.g., `MNIST_MLP`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMLP(object):\n",
    "    def __call__(self, inputs):\n",
    "        net = tf.layers.dense(inputs, 512, activation=tf.nn.relu, name='dense1')\n",
    "        net = tf.layers.dropout(net, rate=0.2, name='drop1')\n",
    "        net = tf.layers.dense(net, 512, activation=tf.nn.relu, name='dense2')\n",
    "        net = tf.layers.dropout(net, rate=0.2, name='drop2')\n",
    "        net = tf.layers.dense(net, NUM_CLASSES, activation=tf.nn.softmax, name='dense3')\n",
    "        return net\n",
    "    \n",
    "class MNIST_MLP(MNIST):\n",
    "    def _load(self, features):\n",
    "        images = tf.reshape(features, [-1, IMG_EDGE**2])\n",
    "        self.learning_rate = 0.001# Learning rate from keras.optimizers.RMSprop\n",
    "        return ModelMLP(), images\n",
    "    def _load_batch_size_limits(self):\n",
    "        self.start_power = 1\n",
    "        self.end_power = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the subclasses implemented, all we need to do is instantiate them and call `get_data` for both. This will train all the models and run all the inferences for all the batch sizes we need, with all possible combinations of machine type (CPU or GPU) and MNIST implementation (MLP or CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory could not be purged.\n",
      "MNIST MLP\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch 1/1: WARNING:tensorflow:Reraising captured error\n",
      "Accuracy: 0.0952 \tLoss: 2.3000283 \tTime: 129.39456272125244 s\n",
      "\n",
      " Batch size: 128 \tTrain time: 0.30113971224698155 \tInference time 8.115811007363455e-08 (7)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "purge_model_files()\n",
    "\n",
    "print(\"MNIST MLP\")\n",
    "print()\n",
    "mlp_tpu = MNIST_MLP()\n",
    "\n",
    "print('+'*100)\n",
    "mlp_tpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With MLP done, we will implement a subclass of `MNIST` using a CNN and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN(object):\n",
    "    def __call__(self, inputs):\n",
    "        net = tf.layers.conv2d(inputs, 32, [5, 5], activation=tf.nn.relu, name='conv1')\n",
    "        net = tf.layers.max_pooling2d(net, [2, 2], 2, name='pool1')\n",
    "        net = tf.layers.conv2d(net, 64, [5, 5], activation=tf.nn.relu, name='conv2')\n",
    "        net = tf.layers.max_pooling2d(net, [2, 2], 2, name='pool2')\n",
    "        net = tf.layers.flatten(net, name='flat')\n",
    "        net = tf.layers.dense(net, NUM_CLASSES, activation=None, name='fc1')\n",
    "        return net\n",
    "    \n",
    "class MNIST_CNN(MNIST):\n",
    "    def _load(self, features):\n",
    "        images = tf.reshape(features, [-1, IMG_EDGE, IMG_EDGE, 1])\n",
    "        self.learning_rate = 1.0# Learning rate from keras.optimizers.Adadelta\n",
    "        return ModelCNN(), images\n",
    "    def _load_batch_size_limits(self):\n",
    "        self.start_power = 1\n",
    "        self.end_power = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory purged\n",
      "MNIST CNN\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch 1/1: WARNING:tensorflow:Reraising captured error\n",
      "Accuracy: 0.1354 \tLoss: 2.2963862 \tTime: 132.0393784046173 s\n",
      "\n",
      " Batch size: 128 \tTrain time: 0.30729372392134235 \tInference time 9.499490261077881e-08 (7)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++ DONE +++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "purge_model_files()\n",
    "\n",
    "print(\"MNIST CNN\")\n",
    "print()\n",
    "cnn_tpu = MNIST_CNN()\n",
    "\n",
    "print('+'*100)\n",
    "cnn_tpu.get_data()\n",
    "\n",
    "print()\n",
    "print('+'*47, \"DONE\", '+'*47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing Data\n",
    "Now we will graph the data we have collected above. First we import the `matplotlib` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_mlp = np.array(mlp_tpu.batch_sizes)\n",
    "x_cnn = np.array(cnn_tpu.batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the train time and inference times on CPUs and GPUs as a function of batch size. This is train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_mlp, mlp_tpu.train_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_cnn, cnn_tpu.train_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Train time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.001, 2])\n",
    "plt.legend(['MLP TPU', 'CNN TPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_mlp, mlp_tpu.inference_times, c='b', alpha = 0.5)\n",
    "plt.scatter(x_cnn, cnn_tpu.inference_times, c='y', alpha = 0.5, marker='^')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axis([1, 10000, 0.0001, 0.1])\n",
    "plt.legend(['MLP TPU', 'CNN TPU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
